# Twitter/X 推文抓取工具 - 使用指南

## 📋 更新内容

本次更新针对 Twitter/X 的反爬虫机制进行了全面优化，主要改进包括：

### ✨ 核心改进

1. **使用 `undetected_chromedriver`**
   - 替代传统的 Selenium WebDriver
   - 自动修补 ChromeDriver 二进制文件，移除自动化检测特征
   - 极大降低被检测为机器人的风险

2. **代理服务器支持**
   - 默认使用本地代理 `http://127.0.0.1:7890`
   - 可以自定义代理或设置为 `None` 禁用
   - 支持规模化抓取，避免单一IP被封锁

3. **持久化登录状态**
   - 使用 Chrome Profile 保存登录信息
   - 首次运行需手动登录，之后自动保持登录状态
   - 避免频繁登录触发风险检测

4. **人类行为模拟**
   - 随机延时（3-7秒）代替固定延时
   - 自动模拟页面滚动行为
   - 5%概率执行"意外"操作（刷新、滚动、鼠标移动）
   - 行为序列不可预测，更像真实用户

5. **断点续传功能**
   - 自动记录已抓取的推文ID
   - 支持中断后继续抓取
   - 避免重复抓取浪费资源

## 🚀 快速开始

### 1. 安装依赖

```bash
pip install -r requirements.txt
```

主要依赖：
- `undetected-chromedriver>=3.5.0` - 反检测浏览器驱动
- `selenium>=4.0.0` - Web自动化框架
- `requests>=2.28.0` - HTTP请求库

### 2. 配置代理（可选）

编辑 `main.py` 文件顶部的配置区域：

```python
# ==================== 配置区域 ====================
# 代理配置（如果不需要代理，设置为None）
PROXY = "http://127.0.0.1:7890"  # 默认代理地址，可以设置为None禁用

# 是否使用持久化Profile（第一次运行需要手动登录，之后会保存登录状态）
USE_PERSISTENT_PROFILE = True
PROFILE_DIR = "./chrome_profile"

# 已抓取推文ID记录文件（避免重复抓取）
SCRAPED_IDS_FILE = "scraped_tweet_ids.json"
```

**代理设置说明：**
- **使用代理**：保持 `PROXY = "http://127.0.0.1:7890"` 或修改为你的代理地址
- **不使用代理**：设置 `PROXY = None`

### 3. 运行脚本

```bash
python main.py
```

### 4. 首次登录

**第一次运行时：**
1. 脚本会自动打开 Chrome 浏览器并跳转到 Twitter/X
2. 检测到未登录状态，会提示你手动登录
3. 在浏览器中完成登录（包括两步验证等）
4. 登录成功后，回到终端按 Enter 键继续
5. 登录信息会自动保存到 `chrome_profile` 目录

**后续运行：**
- 脚本会自动检测到已登录状态
- 直接开始抓取，无需重复登录
- 如需切换账号，删除 `chrome_profile` 目录后重新运行

## 📊 输出文件说明

运行完成后会生成以下文件：

1. **CSV文件** - `tweets_data_YYYYMMDD_HHMMSS.csv`
   - 结构化数据，包含：序号、用户名、发布时间、推文链接、内容、图片数量、图片URL

2. **Markdown文件** - `tweets_data_YYYYMMDD_HHMMSS.md`
   - 可读性更好的文档格式
   - 包含本地图片引用和原图链接

3. **图片目录** - `tweets_images_YYYYMMDD_HHMMSS/`
   - 所有下载的推文图片（原图质量）
   - 命名格式：`tweet_N_img_M.ext`

4. **抓取记录** - `scraped_tweet_ids.json`
   - 已抓取的推文ID列表
   - 用于断点续传和避免重复抓取

## 🎯 反爬虫策略详解

### 1. 浏览器指纹消除
- ✅ 使用 `undetected_chromedriver` 自动patch ChromeDriver
- ✅ 禁用 AutomationControlled 特征
- ✅ 模拟真实浏览器环境

### 2. 行为模式随机化
- ✅ 随机延时 3-7 秒（可在 `random_sleep()` 函数中调整）
- ✅ 每次访问推文后自动滚动 1-3 次
- ✅ 5% 概率执行随机"意外"操作

### 3. 网络层面保护
- ✅ 支持代理服务器，避免IP封锁
- ✅ 推荐使用住宅代理（Residential Proxies）进行大规模抓取

### 4. 会话持久化
- ✅ 保存登录状态，避免频繁登录
- ✅ 降低账号被标记的风险

## ⚠️ 注意事项

1. **代理服务器**
   - 默认使用 `http://127.0.0.1:7890`（常见的本地代理端口）
   - 请确保代理服务正常运行，否则设置为 `None`
   - 大规模抓取建议使用高质量住宅代理

2. **抓取频率**
   - 默认每条推文间隔 3-7 秒随机延时
   - 建议不要短时间内抓取大量推文
   - 可以分批次运行，利用断点续传功能

3. **登录状态**
   - 首次运行需要手动登录
   - `chrome_profile` 目录保存了登录信息，请妥善保管
   - 如遇登录失效，删除该目录重新登录

4. **Chrome浏览器**
   - 确保系统已安装 Chrome 浏览器
   - `undetected_chromedriver` 会自动匹配浏览器版本

5. **合法使用**
   - 请遵守 Twitter/X 的服务条款
   - 仅用于个人学习和研究目的
   - 不要进行商业化大规模抓取

## 🔧 自定义配置

### 调整延时范围

编辑 `random_sleep()` 函数调用：

```python
# 默认 3-7 秒
random_sleep(3, 7)

# 改为 5-10 秒（更保守）
random_sleep(5, 10)

# 改为 2-5 秒（更激进，风险更高）
random_sleep(2, 5)
```

### 调整"意外"操作概率

在主循环中找到：

```python
# 默认 5% 概率
if random.random() < 0.05:

# 改为 10% 概率
if random.random() < 0.10:

# 禁用此功能
if False:  # 或直接删除这段代码
```

### 禁用代理

```python
PROXY = None
```

### 禁用持久化Profile（每次都需要登录）

```python
USE_PERSISTENT_PROFILE = False
```

## 🐛 故障排查

### 问题1: SSL证书错误
**解决方案**: 已改用 `undetected_chromedriver`，通常不会遇到此问题

### 问题2: 登录失败 "Could not log you in now"
**解决方案**: 
- 确保代理服务正常（或设置 `PROXY = None`）
- 使用持久化Profile，避免频繁登录
- 手动完成所有验证步骤后再按Enter

### 问题3: 推文加载超时
**解决方案**:
- 检查网络连接
- 增加等待时间：`WebDriverWait(driver, 30)` 改为更大的值
- 确保代理服务稳定

### 问题4: 图片下载失败
**解决方案**:
- 检查网络连接
- 部分图片可能受限，属于正常情况
- 检查 `requests.get()` 的超时设置

### 问题5: ChromeDriver版本不匹配
**解决方案**: `undetected_chromedriver` 会自动处理版本匹配，无需手动下载

## 📈 性能优化建议

1. **小规模抓取（<100条）**
   - 直接运行即可
   - 可适当缩短延时（风险自负）

2. **中等规模抓取（100-1000条）**
   - 使用代理服务器
   - 分批次运行，利用断点续传
   - 保持默认延时设置

3. **大规模抓取（>1000条）**
   - 必须使用高质量住宅代理
   - 多账号轮换（需修改代码）
   - 增加延时和随机操作概率
   - 分多天分批次进行

## 📝 更新日志

### v2.0 (当前版本)
- ✅ 集成 `undetected_chromedriver`
- ✅ 添加代理服务器支持
- ✅ 实现持久化登录
- ✅ 行为随机化和人类模拟
- ✅ 断点续传功能
- ✅ 智能登录状态检测

### v1.0
- 基础抓取功能
- CSV和Markdown导出
- 图片下载

## 🤝 贡献

如有问题或改进建议，欢迎提出Issue或Pull Request。

## ⚖️ 免责声明

本工具仅供学习和研究使用。使用者应当：
- 遵守相关法律法规
- 遵守 Twitter/X 服务条款
- 不进行恶意或大规模商业化抓取
- 对使用本工具产生的后果自行承担责任

