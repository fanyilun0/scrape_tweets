#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨ç‰¹åˆ—è¡¨ç›‘å¬è„šæœ¬
ç›‘å¬æŒ‡å®šæ¨ç‰¹åˆ—è¡¨ä¸­çš„æ–°æ¨æ–‡ï¼Œå¹¶é€šè¿‡webhookæ¨é€
"""

import re
import time
import os
import requests
import random
import json
import asyncio
import base64
import logging
import sys
import traceback
from datetime import datetime
from io import BytesIO
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException

# ==================== æ—¥å¿—é…ç½® ====================
class ColoredFormatter(logging.Formatter):
    """å½©è‰²æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    COLORS = {
        'DEBUG': '\033[36m',    # é’è‰²
        'INFO': '\033[32m',     # ç»¿è‰²
        'WARNING': '\033[33m',  # é»„è‰²
        'ERROR': '\033[31m',    # çº¢è‰²
        'CRITICAL': '\033[35m', # ç´«è‰²
        'RESET': '\033[0m'      # é‡ç½®
    }
    
    def format(self, record):
        # æ·»åŠ é¢œè‰²
        levelname = record.levelname
        if levelname in self.COLORS:
            record.levelname = f"{self.COLORS[levelname]}{levelname}{self.COLORS['RESET']}"
        return super().format(record)

def setup_logger():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logger = logging.getLogger('ListMonitor')
    logger.setLevel(logging.DEBUG)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG)
    
    # å½©è‰²æ ¼å¼
    formatter = ColoredFormatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(formatter)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆä¿å­˜æ‰€æœ‰æ—¥å¿—ï¼‰
    file_handler = logging.FileHandler('monitor.log', encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

# åˆå§‹åŒ–logger
logger = setup_logger()

# å¯¼å…¥é…ç½®å’Œwebhookæ¨¡å—
try:
    from config import (
        CHROME_PROXY, USE_PERSISTENT_PROFILE, PROFILE_DIR,
        PUSHED_IDS_FILE, LIST_CHECK_INTERVAL, MAX_TWEETS_PER_CHECK,
        TWITTER_LISTS, MONITORED_USERS, ENABLE_USER_FILTER
    )
    from webhook import send_message_async, send_image_async
except ImportError as e:
    logger.critical(f"å¯¼å…¥é…ç½®å¤±è´¥: {e}")
    logger.critical("è¯·ç¡®ä¿ config.py å’Œ webhook.py æ–‡ä»¶å­˜åœ¨")
    exit(1)

# ==================== å¼‚å¸¸å¤„ç† ====================
async def send_error_to_webhook(error_msg, error_detail=""):
    """å‘é€é”™è¯¯ä¿¡æ¯åˆ°webhook
    
    Args:
        error_msg: é”™è¯¯æ¶ˆæ¯
        error_detail: é”™è¯¯è¯¦æƒ…
    """
    try:
        message_parts = [
            "âš ï¸ åˆ—è¡¨ç›‘å¬ç¨‹åºå¼‚å¸¸",
            f"{'='*50}",
            f"âŒ é”™è¯¯: {error_msg}",
        ]
        
        if error_detail:
            message_parts.append(f"\nğŸ“‹ è¯¦æƒ…:\n{error_detail}")
        
        message_parts.append(f"\nâ° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        message = "\n".join(message_parts)
        await send_message_async(message, msg_type="text")
        logger.info("é”™è¯¯ä¿¡æ¯å·²æ¨é€åˆ°webhook")
    except Exception as e:
        logger.error(f"æ¨é€é”™è¯¯ä¿¡æ¯å¤±è´¥: {e}")

# ==================== è¾…åŠ©å‡½æ•° ====================
def load_pushed_ids():
    """åŠ è½½å·²æ¨é€çš„æ¨æ–‡ID"""
    if os.path.exists(PUSHED_IDS_FILE):
        try:
            with open(PUSHED_IDS_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æ¨é€IDå¤±è´¥: {e}")
            return set()
    return set()

def save_pushed_id(tweet_id):
    """ä¿å­˜å·²æ¨é€çš„æ¨æ–‡ID"""
    pushed_ids = load_pushed_ids()
    pushed_ids.add(tweet_id)
    with open(PUSHED_IDS_FILE, 'w', encoding='utf-8') as f:
        json.dump(list(pushed_ids), f, indent=2)

def random_sleep(min_sec=2, max_sec=5):
    """éšæœºå»¶æ—¶ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
    sleep_time = random.uniform(min_sec, max_sec)
    logger.debug(f"éšæœºç­‰å¾… {sleep_time:.2f} ç§’...")
    time.sleep(sleep_time)

def create_undetected_driver():
    """åˆ›å»ºåæ£€æµ‹çš„Chromeæµè§ˆå™¨å®ä¾‹"""
    logger.info("æ­£åœ¨å¯åŠ¨ undetected_chromedriver...")
    
    options = uc.ChromeOptions()
    
    # æ·»åŠ ä»£ç†é…ç½®
    if CHROME_PROXY:
        logger.info(f"é…ç½®ä»£ç†: {CHROME_PROXY}")
        options.add_argument(f'--proxy-server={CHROME_PROXY}')
    
    # ä½¿ç”¨æŒä¹…åŒ–Profile
    if USE_PERSISTENT_PROFILE:
        logger.info(f"ä½¿ç”¨æŒä¹…åŒ–Profile: {PROFILE_DIR}")
        options.add_argument(f'--user-data-dir={PROFILE_DIR}')
    
    # å…¶ä»–ä¼˜åŒ–é€‰é¡¹
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    try:
        driver = uc.Chrome(options=options)
        logger.info("âœ“ undetected_chromedriver å¯åŠ¨æˆåŠŸ")
        return driver
    except Exception as e:
        logger.error(f"å¯åŠ¨å¤±è´¥: {e}")
        raise

def download_image_to_base64(url, proxy=None):
    """ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64ç¼–ç 
    
    Args:
        url: å›¾ç‰‡URL
        proxy: ä»£ç†é…ç½®
    
    Returns:
        str: base64ç¼–ç çš„å›¾ç‰‡ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        proxies = {'http': proxy, 'https': proxy} if proxy else None
        
        response = requests.get(url, headers=headers, proxies=proxies, timeout=15)
        if response.status_code == 200:
            # å°†å›¾ç‰‡æ•°æ®è½¬æ¢ä¸ºbase64
            image_base64 = base64.b64encode(response.content).decode('utf-8')
            return image_base64
        else:
            logger.warning(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            return None
    except Exception as e:
        logger.error(f"ä¸‹è½½å›¾ç‰‡å¼‚å¸¸: {e}")
        return None

def extract_tweet_data(tweet_article):
    """ä»æ¨æ–‡å…ƒç´ ä¸­æå–æ•°æ®ï¼Œèƒ½å¤ŸåŒºåˆ†åŸåˆ›ã€è½¬æ¨ã€å¼•ç”¨å’Œå›å¤
    
    Args:
        tweet_article: Selenium WebElementï¼Œæ¨æ–‡çš„articleå…ƒç´ 
        
    Returns:
        dict: åŒ…å«æ¨æ–‡æ•°æ®çš„å­—å…¸ï¼Œå¦‚æœä¸ç¬¦åˆç™½åå•è¦æ±‚åˆ™è¿”å›None
        
    æ”¯æŒçš„æ¨æ–‡ç±»å‹ï¼š
    - åŸåˆ›æ¨æ–‡ (Original): ç”¨æˆ·å‘å¸ƒçš„åŸå§‹å†…å®¹
    - è½¬æ¨ (Retweet): ç”¨æˆ·è½¬å‘ä»–äººçš„æ¨æ–‡
    - å¼•ç”¨æ¨æ–‡ (Quote): ç”¨æˆ·åœ¨è½¬å‘æ—¶æ·»åŠ äº†è‡ªå·±çš„è¯„è®º
    - å›å¤æ¨æ–‡ (Reply): ç”¨æˆ·å›å¤ä»–äººçš„æ¨æ–‡
    """
    try:
        # æ¨æ–‡ç±»å‹æ ‡è¯†
        is_retweet = False
        is_reply = False
        is_quote = False
        
        # è½¬æ¨è€…ä¿¡æ¯
        retweeter_handle_raw = ""
        retweeter_display_name = ""
        
        # è¢«å¼•ç”¨/å›å¤çš„æ¨æ–‡ä¿¡æ¯
        quoted_tweet_data = None
        reply_to_users = []  # å›å¤çš„ç”¨æˆ·åˆ—è¡¨
        
        # è·å–æ¨æ–‡çš„åŸå§‹HTMLç”¨äºè°ƒè¯• (ä»…åœ¨éœ€è¦æ—¶å¯ç”¨)
        # tweet_html = tweet_article.get_attribute('outerHTML')
        # logger.debug(f"æ¨æ–‡HTMLç‰‡æ®µ: {tweet_html[:200]}...")
        
        # ==================== ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å›å¤æ¨æ–‡ ====================
        # å›å¤æ¨æ–‡ä¼šåœ¨é¡¶éƒ¨æ˜¾ç¤º "Replying to @xxx" æˆ– "å›å¤ @xxx"
        try:
            # æŸ¥æ‰¾å›å¤æ ‡è¯†å…ƒç´ 
            reply_element = tweet_article.find_element(By.XPATH, ".//div[@data-testid='reply']")
            if reply_element:
                is_reply = True
                reply_text = reply_element.text
                
                # æå–è¢«å›å¤çš„ç”¨æˆ·åˆ—è¡¨
                # ä¾‹å¦‚: "Replying to @user1 and @user2"
                reply_links = reply_element.find_elements(By.TAG_NAME, "a")
                for link in reply_links:
                    href = link.get_attribute('href')
                    if href and '/status/' not in href:
                        username = href.split('/')[-1].split('?')[0].split('#')[0]
                        if username and username not in reply_to_users:
                            reply_to_users.append(username)
                
                logger.debug(f"æ£€æµ‹åˆ°å›å¤æ¨æ–‡ï¼Œå›å¤ç»™: {', '.join(['@' + u for u in reply_to_users])}")
        except NoSuchElementException:
            # æ²¡æœ‰å›å¤æ ‡è¯†ï¼Œä¸æ˜¯å›å¤æ¨æ–‡
            pass
        
        # ==================== ç¬¬äºŒæ­¥ï¼šæ£€æµ‹è½¬æ¨/è½¬å¸– ====================
        # æ³¨æ„ï¼šè½¬æ¨å’Œå¼•ç”¨æ˜¯äº’æ–¥çš„ï¼Œè½¬æ¨ä¸èƒ½æœ‰è‡ªå·±çš„è¯„è®º
        try:
            # ç­–ç•¥1: æ£€æŸ¥socialContext (æœ€å¸¸è§)
            # æ ¹æ®HTMLç»“æ„: <span data-testid="socialContext">ä½ å·²è½¬å¸–</span>
            social_context = tweet_article.find_element(By.XPATH, ".//span[@data-testid='socialContext']")
            context_text = social_context.text
            
            # è½¬æ¨/è½¬å¸–å…³é”®è¯åˆ—è¡¨ (æ”¯æŒå¤šè¯­è¨€)
            retweet_keywords = [
                "Retweeted", "è½¬æ¨äº†", "å·²è½¬æ¨", "å·²è½¬å¸–", "ä½ å·²è½¬å¸–",
                "reposted", "è½¬å‘äº†", "retweet", "Retweet",
                "Reposted", "You reposted"  # X çš„æ–°æœ¯è¯­
            ]
            matched_keyword = None
            
            for keyword in retweet_keywords:
                if keyword.lower() in context_text.lower():  # ä¸åŒºåˆ†å¤§å°å†™
                    matched_keyword = keyword
                    is_retweet = True
                    break
            
            if is_retweet:
                # æå–è½¬æ¨è€…çš„ç”¨æˆ·å
                try:
                    # æ ¹æ®HTMLç»“æ„åˆ†æ:
                    # <a href="/Jiangha52202194">
                    #   <span data-testid="socialContext">ä½ å·²è½¬å¸–</span>
                    # </a>
                    # æ‰€ä»¥ socialContext çš„çˆ¶å…ƒç´ å°±æ˜¯ <a> æ ‡ç­¾
                    
                    # æ–¹æ³•1: ä» socialContext çš„çˆ¶çº§ <a> æ ‡ç­¾ä¸­æå–ç”¨æˆ·å
                    try:
                        parent_link = social_context.find_element(By.XPATH, "./parent::a")
                        href = parent_link.get_attribute('href')
                        if href and '/status/' not in href and '/photo/' not in href:
                            retweeter_handle_raw = href.split('/')[-1].split('?')[0].split('#')[0]
                            logger.debug(f"ä»çˆ¶é“¾æ¥æå–åˆ°è½¬æ¨è€…: @{retweeter_handle_raw}")
                    except NoSuchElementException:
                        # å¦‚æœç›´æ¥çˆ¶å…ƒç´ ä¸æ˜¯ <a>ï¼Œå°è¯•å¾€ä¸ŠæŸ¥æ‰¾
                        try:
                            ancestor_link = social_context.find_element(By.XPATH, "./ancestor::a[contains(@href, '/')]")
                            href = ancestor_link.get_attribute('href')
                            if href and '/status/' not in href and '/photo/' not in href:
                                retweeter_handle_raw = href.split('/')[-1].split('?')[0].split('#')[0]
                                logger.debug(f"ä»ç¥–å…ˆé“¾æ¥æå–åˆ°è½¬æ¨è€…: @{retweeter_handle_raw}")
                        except NoSuchElementException:
                            pass
                    
                    # æ–¹æ³•2: æå–æ˜¾ç¤ºåç§°
                    # æ¸…ç†æ˜¾ç¤ºåç§°ï¼Œç§»é™¤"å·²è½¬å¸–"ç­‰æ–‡å­—å’Œå¤šä½™ç©ºæ ¼
                    retweeter_display_name = context_text
                    if matched_keyword:
                        retweeter_display_name = retweeter_display_name.replace(matched_keyword, "").strip()
                    # ç§»é™¤å…¶ä»–å¯èƒ½çš„å¹²æ‰°æ–‡æœ¬
                    retweeter_display_name = re.sub(r'\s+', ' ', retweeter_display_name).strip()
                    
                    # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯æ²¡æœ‰æå–åˆ°ç”¨æˆ·åï¼Œå°è¯•ä»åŒçº§åŒºåŸŸæŸ¥æ‰¾
                    # è¿™ç§æƒ…å†µä¸‹ï¼ŒsocialContext å¯èƒ½å’Œç”¨æˆ·é“¾æ¥åœ¨åŒä¸€ä¸ªåŒºåŸŸ
                    if not retweeter_handle_raw:
                        try:
                            # æŸ¥æ‰¾åŒ…å« socialContext çš„æ•´ä¸ªåŒºåŸŸ
                            context_area = social_context.find_element(By.XPATH, "./ancestor::div[@class='css-175oi2r r-1iusvr4 r-16y2uox']")
                            area_links = context_area.find_elements(By.TAG_NAME, "a")
                            for link in area_links:
                                href = link.get_attribute('href')
                                if href and '/status/' not in href and '/photo/' not in href:
                                    retweeter_handle_raw = href.split('/')[-1].split('?')[0].split('#')[0]
                                    if retweeter_handle_raw:
                                        logger.debug(f"ä»åŒçº§åŒºåŸŸæå–åˆ°è½¬æ¨è€…: @{retweeter_handle_raw}")
                                        break
                        except:
                            pass
                    
                except Exception as e:
                    logger.warning(f"æå–è½¬æ¨è€…ä¿¡æ¯å¤±è´¥: {e}")
                
                # è¡¥å……ï¼šå¦‚æœä»ç„¶æ²¡æœ‰æå–åˆ°è½¬æ¨è€…ç”¨æˆ·å
                # åœ¨"ä½ å·²è½¬å¸–"çš„æƒ…å†µä¸‹ï¼Œè½¬æ¨è€…å°±æ˜¯å½“å‰ç™»å½•ç”¨æˆ·
                # æ¨æ–‡çš„åŸä½œè€…ä¿¡æ¯ä¼šåœ¨åé¢çš„User-Nameä¸­æå–
                if not retweeter_handle_raw:
                    logger.debug("æœªèƒ½æå–è½¬æ¨è€…ç”¨æˆ·åï¼Œå¯èƒ½æ˜¯'ä½ å·²è½¬å¸–'çš„æƒ…å†µï¼ˆè½¬æ¨è€…æ˜¯å½“å‰ç™»å½•ç”¨æˆ·ï¼‰")
                    
        except NoSuchElementException:
            # æ²¡æœ‰socialContext spanï¼Œè¯´æ˜ä¸æ˜¯è½¬æ¨
            # ä½†è¿˜éœ€è¦æ£€æŸ¥æŒ‰é’®çŠ¶æ€
            try:
                # ç­–ç•¥2: æ£€æŸ¥unretweetæŒ‰é’® (å¤‡ç”¨æ–¹æ¡ˆ)
                # <button data-testid="unretweet"> è¡¨ç¤ºå·²è½¬å¸–çŠ¶æ€
                unretweet_button = tweet_article.find_element(By.XPATH, ".//button[@data-testid='unretweet']")
                if unretweet_button:
                    is_retweet = True
                    logger.debug("é€šè¿‡unretweetæŒ‰é’®æ£€æµ‹åˆ°è½¬å¸–")
            except NoSuchElementException:
                is_retweet = False
        
        # ==================== ç¬¬ä¸‰æ­¥ï¼šæå–ç”¨æˆ·ä¿¡æ¯ ====================
        # æå–ç”¨æˆ·åå’Œæ˜¾ç¤ºåç§°ï¼ˆè¿™æ˜¯æ¨æ–‡ä½œè€…çš„ä¿¡æ¯ï¼‰
        user_handle_raw = ""
        user_display_name = ""
        try:
            user_name_div = tweet_article.find_element(By.XPATH, ".//div[@data-testid='User-Name']")
            
            # æå–handle (@username) - ä¼˜å…ˆçº§æ›´é«˜ï¼Œå› ä¸ºæ›´å‡†ç¡®
            try:
                # å°è¯•å¤šç§æ–¹æ³•æå–ç”¨æˆ·å
                handle_links = user_name_div.find_elements(By.XPATH, ".//a[contains(@href, '/')]")
                for link in handle_links:
                    href = link.get_attribute('href')
                    if href and '/status/' not in href and '/photo/' not in href:
                        # æå–ç”¨æˆ·åï¼Œç¡®ä¿æ¸…ç†æ‰€æœ‰å¤šä½™å‚æ•°
                        user_handle_raw = href.split('/')[-1].split('?')[0].split('#')[0]
                        if user_handle_raw:  # æ‰¾åˆ°äº†å°±é€€å‡º
                            break
                
            except Exception as e:
                logger.warning(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
            
            # æå–æ˜¾ç¤ºåç§°
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨
                display_name_element = None
                
                # æ–¹æ³•1: é€šè¿‡CSSç±»å
                try:
                    display_name_element = user_name_div.find_element(By.XPATH, ".//span[contains(@class, 'css-1jxf684')]")
                except:
                    pass
                
                # æ–¹æ³•2: é€šè¿‡ç»“æ„æŸ¥æ‰¾ (backup)
                if not display_name_element:
                    try:
                        spans = user_name_div.find_elements(By.TAG_NAME, "span")
                        for span in spans:
                            text = span.text.strip()
                            # æ˜¾ç¤ºåç§°é€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªé@å¼€å¤´çš„æ–‡æœ¬
                            if text and not text.startswith('@') and len(text) > 0:
                                display_name_element = span
                                break
                    except:
                        pass
                
                if display_name_element:
                    user_display_name = display_name_element.text.strip()
                    
            except Exception as e:
                logger.warning(f"æå–æ˜¾ç¤ºåç§°å¤±è´¥: {e}")
                
        except NoSuchElementException:
            #logger.debug("æœªæ‰¾åˆ°User-Nameå…ƒç´ ")
            pass
        
        # ==================== ç¬¬å››æ­¥ï¼šç™½åå•è¿‡æ»¤ ====================
        # åªè¦ è½¬æ¨è€… æˆ– åŸä½œè€… ä»»ä½•ä¸€æ–¹åœ¨ç™½åå•å†…ï¼Œå°±é€šè¿‡
        if ENABLE_USER_FILTER:
            is_author_in_list = is_user_in_whitelist(user_handle_raw)
            is_retweeter_in_list = False
            
            if is_retweet:
                is_retweeter_in_list = is_user_in_whitelist(retweeter_handle_raw)

            # æ ¸å¿ƒåˆ¤æ–­ï¼š
            # 1. å¦‚æœæ˜¯åŸåˆ›æ¨æ–‡ã€å›å¤æˆ–å¼•ç”¨ï¼Œä½œè€…å¿…é¡»åœ¨åˆ—è¡¨å†…
            # 2. å¦‚æœæ˜¯è½¬æ¨ï¼Œè½¬æ¨è€… æˆ– åŸä½œè€… è‡³å°‘æœ‰ä¸€ä¸ªåœ¨åˆ—è¡¨å†…
            if not is_author_in_list and not is_retweeter_in_list:
                # logger.debug(f"è·³è¿‡: è½¬æ¨è€… @{retweeter_handle_raw} å’ŒåŸä½œè€… @{user_handle_raw} å‡ä¸åœ¨ç™½åå•")
                return None  # åªæœ‰å½“ä¸¤è€…éƒ½ä¸åœ¨ç™½åå•æ—¶ï¼Œæ‰è·³è¿‡
        # ==================== ç™½åå•è¿‡æ»¤ç»“æŸ ====================
        
        
        # éªŒè¯æå–çš„ç”¨æˆ·ä¿¡æ¯
        if is_retweet:
            if not retweeter_handle_raw:
                logger.warning("è½¬æ¨æ£€æµ‹æˆåŠŸï¼Œä½†æœªèƒ½æå–åˆ°è½¬æ¨è€…ç”¨æˆ·åï¼Œå¯èƒ½å­˜åœ¨è§£æé—®é¢˜")
            if not user_handle_raw:
                logger.warning("è½¬æ¨æ£€æµ‹æˆåŠŸï¼Œä½†æœªèƒ½æå–åˆ°åŸä½œè€…ç”¨æˆ·åï¼Œå¯èƒ½å­˜åœ¨è§£æé—®é¢˜")
        else:
            if not user_handle_raw:
                logger.warning("åŸåˆ›æ¨æ–‡æœªèƒ½æå–åˆ°ä½œè€…ç”¨æˆ·åï¼Œå¯èƒ½å­˜åœ¨è§£æé—®é¢˜")
        
        # æå–æ¨æ–‡IDå’ŒURL
        tweet_link_elements = tweet_article.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
        tweet_url = None
        tweet_id = None
        
        for link in tweet_link_elements:
            href = link.get_attribute('href')
            if '/status/' in href and '/analytics' not in href:  # æ’é™¤analyticsé“¾æ¥
                tweet_url = href
                # æ¸…ç†URLï¼Œç§»é™¤é¢å¤–çš„è·¯å¾„
                tweet_id = href.split('/status/')[-1].split('?')[0].split('/')[0]
                break
        
        if not tweet_id:
            return None
        
        # æå–æ¨æ–‡æ­£æ–‡
        tweet_text = ""
        try:
            tweet_text_element = tweet_article.find_element(By.XPATH, ".//div[@data-testid='tweetText']")
            tweet_text = tweet_text_element.text
        except NoSuchElementException:
            pass
        
        # ==================== ç¬¬äº”æ­¥ï¼šæå–å¼•ç”¨æ¨æ–‡ï¼ˆQuote Tweetï¼‰====================
        # å¼•ç”¨æ¨æ–‡ï¼šç”¨æˆ·åœ¨è½¬å‘æ—¶æ·»åŠ äº†è‡ªå·±çš„è¯„è®ºï¼Œä¼šåµŒå¥—æ˜¾ç¤ºåŸæ¨æ–‡
        # æ³¨æ„ï¼šåªæœ‰éè½¬æ¨çš„æ¨æ–‡æ‰å¯èƒ½æ˜¯å¼•ç”¨æ¨æ–‡ï¼ˆè½¬æ¨å’Œå¼•ç”¨æ˜¯äº’æ–¥çš„ï¼‰
        if not is_retweet:
            try:
                # åŸºäºHTMLç»“æ„åˆ†æï¼Œä½¿ç”¨ä¸€ä¸ªéå¸¸ç¨³å®šå’Œç²¾å‡†çš„XPathæ¥å®šä½å¼•ç”¨æ¨æ–‡çš„å®¹å™¨
                # ç­–ç•¥ï¼šå¯»æ‰¾ä¸€ä¸ª role="link" çš„divï¼Œä¸”å…¶å†…éƒ¨å¿…é¡»åŒ…å«ä¸€ä¸ª 'User-Name' çš„æµ‹è¯•ID
                # è¿™ä¸ªæ–¹æ³•å¯ä»¥ç²¾å‡†åœ°ç­›é€‰å‡ºçœŸæ­£çš„å¼•ç”¨æ¨æ–‡ï¼Œæ’é™¤å¹¿å‘Šã€é“¾æ¥å¡ç‰‡ç­‰å¹²æ‰°é¡¹
                quoted_container_xpath = ".//div[@role='link' and .//div[@data-testid='User-Name']]"
                quoted_container = tweet_article.find_element(By.XPATH, quoted_container_xpath)
                
                if quoted_container:
                    is_quote = True
                    # æå–è¢«å¼•ç”¨æ¨æ–‡çš„ä¿¡æ¯
                    quoted_author_handle = ""
                    quoted_author_name = ""
                    quoted_text = ""
                    quoted_tweet_url = ""

                    # æå–ä½œè€…ä¿¡æ¯
                    try:
                        quoted_user_div = quoted_container.find_element(By.XPATH, ".//div[@data-testid='User-Name']")
                        
                        # æå–handle (@username) - ä»é“¾æ¥ä¸­æå–æœ€å¯é 
                        try:
                            handle_links = quoted_user_div.find_elements(By.XPATH, ".//a[contains(@href, '/')]")
                            for link in handle_links:
                                href = link.get_attribute('href')
                                if href and '/status/' not in href and '/photo/' not in href:
                                    # æ¸…ç†URLå‚æ•°ï¼Œæå–çº¯ç”¨æˆ·å
                                    quoted_author_handle = href.split('/')[-1].split('?')[0].split('#')[0]
                                    if quoted_author_handle:
                                        break
                        except NoSuchElementException:
                            pass
                        
                        # å¦‚æœä»é“¾æ¥æå–å¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­è§£æ @username
                        if not quoted_author_handle:
                            spans = quoted_user_div.find_elements(By.TAG_NAME, "span")
                            for span in spans:
                                text = span.text.strip()
                                if text.startswith('@'):
                                    quoted_author_handle = text[1:]  # ç§»é™¤ @ ç¬¦å·
                                    break
                        
                        # æå–æ˜¾ç¤ºåç§°
                        spans = quoted_user_div.find_elements(By.TAG_NAME, "span")
                        for span in spans:
                            text = span.text.strip()
                            # ç¬¬ä¸€ä¸ªä¸ä»¥@å¼€å¤´ä¸”ä¸ä¸ºç©ºçš„spané€šå¸¸å°±æ˜¯æ˜¾ç¤ºåç§°
                            if text and not text.startswith('@') and len(text) > 0:
                                quoted_author_name = text
                                break
                    except Exception as e:
                        logger.debug(f"æå–è¢«å¼•ç”¨æ¨æ–‡çš„ä½œè€…å¤±è´¥: {e}")

                    # æå–æ­£æ–‡ï¼ˆå¯èƒ½ä¸ºç©ºï¼Œå› ä¸ºæœ‰äº›å¼•ç”¨æ¨æ–‡åªæœ‰å›¾ç‰‡ï¼‰
                    try:
                        quoted_text_div = quoted_container.find_element(By.XPATH, ".//div[@data-testid='tweetText']")
                        quoted_text = quoted_text_div.text
                    except NoSuchElementException:
                        # æœ‰äº›å¼•ç”¨æ¨æ–‡æ²¡æœ‰æ–‡å­—å†…å®¹ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                        logger.debug("è¢«å¼•ç”¨æ¨æ–‡æ²¡æœ‰æ–‡æœ¬å†…å®¹")
                    except Exception as e:
                        logger.debug(f"æå–è¢«å¼•ç”¨æ¨æ–‡çš„æ­£æ–‡å¤±è´¥: {e}")
                    
                    # æå–è¢«å¼•ç”¨æ¨æ–‡çš„URL
                    try:
                        quoted_links = quoted_container.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
                        for link in quoted_links:
                            href = link.get_attribute('href')
                            if href and '/status/' in href and '/analytics' not in href:
                                quoted_tweet_url = href.split('?')[0]
                                break
                    except Exception:
                        pass
                    
                    # å¦‚æœæˆåŠŸæå–åˆ°ä¿¡æ¯ï¼Œå°±ç»„è£…èµ·æ¥
                    if quoted_author_handle or quoted_text:
                        quoted_tweet_data = {
                            "author_handle": quoted_author_handle,
                            "author_display_name": quoted_author_name,
                            "text": quoted_text,
                            "url": quoted_tweet_url
                        }
                        logger.debug(f"æ£€æµ‹åˆ°å¼•ç”¨æ¨æ–‡: @{quoted_author_handle} - {quoted_text[:30] if quoted_text else 'æ— æ–‡æœ¬'}...")
                    
            except NoSuchElementException:
                # æ²¡æ‰¾åˆ°å®¹å™¨ï¼Œè¯´æ˜è¿™ä¸æ˜¯ä¸€æ¡å¼•ç”¨æ¨æ–‡ï¼Œå±äºæ­£å¸¸æƒ…å†µ
                pass
            except Exception as e:
                logger.debug(f"å°è¯•æå–å¼•ç”¨æ¨æ–‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        # ==================== æå–å¼•ç”¨æ¨æ–‡ç»“æŸ ====================
        
        # æå–æ¨æ–‡æ—¶é—´
        tweet_time = ""
        try:
            time_element = tweet_article.find_element(By.XPATH, ".//time")
            tweet_time = time_element.get_attribute('datetime')
            # æ ¼å¼åŒ–æ—¶é—´ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
            if tweet_time:
                dt = datetime.fromisoformat(tweet_time.replace('Z', '+00:00'))
                tweet_time = dt.strftime('%Y-%m-%d %H:%M:%S')
        except NoSuchElementException:
            pass
        
        # æå–å›¾ç‰‡URL
        image_urls = []
        try:
            photo_divs = tweet_article.find_elements(By.XPATH, ".//div[@data-testid='tweetPhoto']")
            
            for photo_div in photo_divs:
                img_elements = photo_div.find_elements(By.TAG_NAME, "img")
                for img_element in img_elements:
                    img_url = img_element.get_attribute('src')
                    
                    if img_url:
                        # å°†URLå‚æ•°æ›¿æ¢ä¸º 'name=orig' æ¥è·å–æœ€é«˜æ¸…çš„åŸå›¾
                        if 'name=' in img_url:
                            orig_img_url = re.sub(r'name=\w+', 'name=orig', img_url)
                        else:
                            orig_img_url = img_url.split('?')[0] + '?format=jpg&name=orig'
                        
                        if orig_img_url not in image_urls:
                            image_urls.append(orig_img_url)
        except:
            pass
        
        # ==================== ç¬¬å…­æ­¥ï¼šæ„å»ºè¿”å›æ•°æ® ====================
        tweet_data = {
            "id": tweet_id,
            "url": tweet_url,
            
            # æ¨æ–‡ç±»å‹æ ‡è¯†
            "is_retweet": is_retweet,    # æ˜¯å¦ä¸ºè½¬æ¨
            "is_reply": is_reply,        # æ˜¯å¦ä¸ºå›å¤
            "is_quote": is_quote,        # æ˜¯å¦ä¸ºå¼•ç”¨æ¨æ–‡
            
            # ç”¨æˆ·ä¿¡æ¯
            "retweeter": {  # è½¬æ¨è€…ä¿¡æ¯ï¼ˆä»…è½¬æ¨æ—¶æœ‰æ•ˆï¼‰
                "handle_raw": retweeter_handle_raw,
                "display_name": retweeter_display_name
            },
            "author": {  # æ¨æ–‡ä½œè€…ä¿¡æ¯
                "handle_raw": user_handle_raw,
                "display_name": user_display_name
            },
            
            # å›å¤ä¿¡æ¯
            "reply_to": reply_to_users,  # å›å¤çš„ç”¨æˆ·åˆ—è¡¨
            
            # å¼•ç”¨æ¨æ–‡ä¿¡æ¯
            "quoted": quoted_tweet_data,  # è¢«å¼•ç”¨çš„æ¨æ–‡ä¿¡æ¯ï¼ˆä»…å¼•ç”¨æ¨æ–‡æ—¶æœ‰æ•ˆï¼‰
            
            # æ¨æ–‡å†…å®¹
            "time": tweet_time,
            "text": tweet_text,
            "images": image_urls
        }
        
        return tweet_data
        
    except Exception as e:
        logger.warning(f"æå–æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
        return None

async def send_tweet_to_webhook(tweet_data):
    """å°†æ¨æ–‡æ•°æ®å‘é€åˆ°webhookï¼Œä¸ºè½¬æ¨ã€å¼•ç”¨å’Œå›å¤æä¾›ä¸“é—¨æ ¼å¼
    
    Args:
        tweet_data: æ¨æ–‡æ•°æ®å­—å…¸
    """
    try:
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        message_parts = []
        
        # æå–å…³é”®ä¿¡æ¯
        is_retweet = tweet_data.get('is_retweet', False)
        is_reply = tweet_data.get('is_reply', False)
        is_quote = tweet_data.get('is_quote', False)
        quoted_data = tweet_data.get('quoted')
        reply_to_users = tweet_data.get('reply_to', [])
        
        # æ ¹æ®æ¨æ–‡ç±»å‹ç”Ÿæˆä¸åŒæ ¼å¼çš„æ¶ˆæ¯
        if is_retweet:
            # ========== è½¬æ¨æ¨æ–‡ ==========
            message_parts.append(f"ğŸ”„ ç”¨æˆ·è½¬æ¨æé†’")
            retweeter = tweet_data['retweeter']
            author = tweet_data['author']
            
            # çªå‡ºè½¬æ¨è€…
            if retweeter['display_name']:
                message_parts.append(f"ğŸ‘¤ è½¬æ¨è€…: {retweeter['display_name']} (@{retweeter['handle_raw']})")
            else:
                message_parts.append(f"ğŸ‘¤ è½¬æ¨è€…: @{retweeter['handle_raw']}")
            
            # æ ‡æ˜åŸä½œè€…
            if author['display_name']:
                message_parts.append(f"âœï¸ åŸä½œè€…: {author['display_name']} (@{author['handle_raw']})")
            else:
                message_parts.append(f"âœï¸ åŸä½œè€…: @{author['handle_raw']}")
        
        elif is_quote:
            # ========== å¼•ç”¨æ¨æ–‡ ==========
            message_parts.append(f"ğŸ“– æ–°çš„å¼•ç”¨æ¨æ–‡")
            author = tweet_data['author']
            
            if author['display_name']:
                message_parts.append(f"ğŸ‘¤ ä½œè€…: {author['display_name']} (@{author['handle_raw']})")
            else:
                message_parts.append(f"ğŸ‘¤ ä½œè€…: @{author['handle_raw']}")
        
        elif is_reply:
            # ========== å›å¤æ¨æ–‡ ==========
            message_parts.append(f"ğŸ’¬ æ–°çš„å›å¤æ¨æ–‡")
            author = tweet_data['author']
            
            if author['display_name']:
                message_parts.append(f"ğŸ‘¤ ä½œè€…: {author['display_name']} (@{author['handle_raw']})")
            else:
                message_parts.append(f"ğŸ‘¤ ä½œè€…: @{author['handle_raw']}")
            
            # æ ‡æ˜å›å¤çš„å¯¹è±¡
            if reply_to_users:
                reply_to_str = ", ".join([f"@{u}" for u in reply_to_users])
                message_parts.append(f"â†©ï¸ å›å¤ç»™: {reply_to_str}")
        
        else:
            # ========== åŸåˆ›æ¨æ–‡ ==========
            message_parts.append(f"ğŸ¦ æ–°æ¨æ–‡ç›‘å¬")
            author = tweet_data['author']
            
            if author['display_name']:
                message_parts.append(f"ğŸ‘¤ ä½œè€…: {author['display_name']} (@{author['handle_raw']})")
            else:
                message_parts.append(f"ğŸ‘¤ ä½œè€…: @{author['handle_raw']}")
        
        # æ—¶é—´
        if tweet_data['time']:
            message_parts.append(f"ğŸ• æ—¶é—´: {tweet_data['time']}")
        
        message_parts.append(f"")
        
        # æ¨æ–‡å†…å®¹
        if tweet_data['text']:
            message_parts.append(f"ğŸ“ å†…å®¹:")
            message_parts.append(tweet_data['text'])
            message_parts.append(f"")
        
        # ==================== å±•ç¤ºè¢«å¼•ç”¨çš„æ¨æ–‡å†…å®¹ ====================
        if quoted_data:
            # æ·»åŠ ä¸€ä¸ªæ¼‚äº®çš„åˆ†å‰²çº¿å’Œæ ‡é¢˜
            message_parts.append(" å¼•ç”¨çš„æ¨æ–‡:\n ")
            
            # æ ¼å¼åŒ–è¢«å¼•ç”¨æ¨æ–‡çš„ä½œè€…ä¿¡æ¯
            quoted_author_info = f"@{quoted_data['author_handle']}"
            if quoted_data['author_display_name']:
                quoted_author_info = f"{quoted_data['author_display_name']} ({quoted_author_info})"
            
            message_parts.append(f"ğŸ—£ï¸  {quoted_author_info} è¯´:")
            
            # ä½¿ç”¨å¼•ç”¨æ ¼å¼æ¥å±•ç¤ºåŸæ–‡
            original_text = quoted_data.get('text', '[å†…å®¹ä¸ºç©º]')
            quoted_text_formatted = "\n".join([f"> {line}" for line in original_text.split('\n')])
            message_parts.append(quoted_text_formatted)
            
            # å¦‚æœæœ‰è¢«å¼•ç”¨æ¨æ–‡çš„URLï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
            if quoted_data.get('url'):
                message_parts.append(f"> ğŸ”— {quoted_data['url']}")
            
        # ==================== å¼•ç”¨æ¨æ–‡å†…å®¹å±•ç¤ºç»“æŸ ====================
        
        # æ¨æ–‡é“¾æ¥
        if tweet_data['url']:
            message_parts.append(f"ğŸ”— é“¾æ¥: {tweet_data['url']}")
        
        # å›¾ç‰‡ä¿¡æ¯
        if tweet_data['images']:
            message_parts.append(f"ğŸ–¼ï¸ åŒ…å« {len(tweet_data['images'])} å¼ å›¾ç‰‡")
        
        # å‘é€æ–‡æœ¬æ¶ˆæ¯
        message = "\n".join(message_parts)
        logger.info(f"å‘é€æ–‡æœ¬æ¶ˆæ¯: {message}")
        await send_message_async(message, msg_type="text")
        
        # å‘é€å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if tweet_data['images']:
            for idx, img_url in enumerate(tweet_data['images'], 1):
                # ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
                proxy = CHROME_PROXY if CHROME_PROXY else None
                image_base64 = download_image_to_base64(img_url, proxy)
                
                if image_base64:
                    success = await send_image_async(image_base64=image_base64)
                    if not success:
                        logger.warning(f"å›¾ç‰‡ {idx}/{len(tweet_data['images'])} å‘é€å¤±è´¥")
                    
                    # å›¾ç‰‡å‘é€é—´éš”
                    if idx < len(tweet_data['images']):
                        await asyncio.sleep(1)
                else:
                    logger.warning(f"å›¾ç‰‡ {idx}/{len(tweet_data['images'])} ä¸‹è½½å¤±è´¥")
        
        return True
        
    except Exception as e:
        logger.error(f"æ¨é€æ¨æ–‡å¤±è´¥: {e}")
        error_detail = traceback.format_exc()
        await send_error_to_webhook("æ¨é€æ¨æ–‡åˆ°webhookå¤±è´¥", error_detail)
        return False

def is_user_in_whitelist(user_handle_raw):
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨ç™½åå•ä¸­
    
    Args:
        user_handle_raw: ç”¨æˆ·åï¼ˆä¸å«@ï¼‰
        
    Returns:
        bool: å¦‚æœç”¨æˆ·åœ¨ç™½åå•æˆ–æœªå¯ç”¨è¿‡æ»¤ï¼Œè¿”å›True
    """
    # å¦‚æœæœªå¯ç”¨ç”¨æˆ·è¿‡æ»¤ï¼Œè¿”å›Trueï¼ˆå…è®¸æ‰€æœ‰ç”¨æˆ·ï¼‰
    if not ENABLE_USER_FILTER:
        return True
    
    # å¦‚æœç™½åå•ä¸ºç©ºï¼Œè¿”å›Trueï¼ˆå…è®¸æ‰€æœ‰ç”¨æˆ·ï¼‰
    if not MONITORED_USERS:
        return True
    
    # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨ç™½åå•ä¸­ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    return user_handle_raw.lower() in [u.lower() for u in MONITORED_USERS]

def scrape_list_tweets(driver, list_url, max_tweets=20):
    """æŠ“å–æ¨ç‰¹åˆ—è¡¨ä¸­çš„æ¨æ–‡
    
    Args:
        driver: Selenium WebDriverå®ä¾‹
        list_url: æ¨ç‰¹åˆ—è¡¨URL
        max_tweets: æœ€å¤šæŠ“å–çš„æ¨æ–‡æ•°é‡
        
    Returns:
        list: æ¨æ–‡æ•°æ®åˆ—è¡¨
    """
    logger.info(f"è®¿é—®åˆ—è¡¨: {list_url}")
    
    try:
        driver.get(list_url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        wait = WebDriverWait(driver, 15)
        
        # ç­‰å¾…æ¨æ–‡åŠ è½½
        logger.info("ç­‰å¾…æ¨æ–‡åŠ è½½...")
        time.sleep(5)
        
        tweets_data = []
        processed_ids = set()
        no_new_tweets_count = 0  # è¿ç»­æœªå‘ç°æ–°æ¨æ–‡çš„æ¬¡æ•°
        max_no_new_tweets = 3     # æœ€å¤šå…è®¸3æ¬¡æœªå‘ç°æ–°æ¨æ–‡
        
        # å…ˆè·å–å½“å‰å¯è§çš„æ¨æ–‡ï¼ˆä¸æ»šåŠ¨ï¼‰
        logger.info("æŠ“å–å½“å‰å¯è§æ¨æ–‡...")
        tweet_articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
        logger.info(f"æ‰¾åˆ° {len(tweet_articles)} æ¡å¯è§æ¨æ–‡")
        
        for article in tweet_articles:
            if len(tweets_data) >= max_tweets:
                break
            
            # extract_tweet_data ç°åœ¨ä¼šåœ¨å†…éƒ¨è¿›è¡Œç™½åå•è¿‡æ»¤ï¼Œè¿”å›Noneè¡¨ç¤ºä¸ç¬¦åˆè¦æ±‚
            tweet_data = extract_tweet_data(article)
            
            if tweet_data and tweet_data['id'] not in processed_ids:
                processed_ids.add(tweet_data['id'])
                tweets_data.append(tweet_data)
                
                # æ ¹æ®æ¨æ–‡ç±»å‹æ˜¾ç¤ºä¸åŒçš„æ—¥å¿—
                author = tweet_data['author']['handle_raw']
                if tweet_data['is_retweet']:
                    logger.info(f"âœ“ æå–è½¬æ¨: @{tweet_data['retweeter']['handle_raw']} RT @{author} - {tweet_data['id']}")
                elif tweet_data['is_quote']:
                    quoted_data = tweet_data.get('quoted')
                    quoted_author = quoted_data.get('author_handle', 'unknown') if quoted_data else 'unknown'
                    logger.info(f"âœ“ æå–å¼•ç”¨: @{author} å¼•ç”¨ @{quoted_author} - {tweet_data['id']}")
                elif tweet_data['is_reply']:
                    reply_to = tweet_data.get('reply_to', [])
                    reply_str = ', '.join([f"@{u}" for u in reply_to]) if reply_to else '?'
                    logger.info(f"âœ“ æå–å›å¤: @{author} å›å¤ {reply_str} - {tweet_data['id']}")
                else:
                    logger.info(f"âœ“ æå–åŸåˆ›: @{author} - {tweet_data['id']}")
        
        # å¦‚æœéœ€è¦æ›´å¤šæ¨æ–‡ï¼Œæ‰è¿›è¡Œæ»šåŠ¨
        if len(tweets_data) < max_tweets:
            logger.info(f"å½“å‰è·å– {len(tweets_data)} æ¡ï¼Œéœ€è¦æ›´å¤šæ¨æ–‡ï¼Œå¼€å§‹æ»šåŠ¨...")
            
            while len(tweets_data) < max_tweets and no_new_tweets_count < max_no_new_tweets:
                # è®°å½•æ»šåŠ¨å‰çš„æ¨æ–‡æ•°é‡
                before_scroll_count = len(tweets_data)
                
                # æ»šåŠ¨é¡µé¢
                driver.execute_script("window.scrollBy(0, 800);")  # æ¯æ¬¡åªæ»šåŠ¨800åƒç´ ï¼Œé¿å…è·³è¿‡å†…å®¹
                time.sleep(2)
                
                # è·å–æ–°å‡ºç°çš„æ¨æ–‡
                tweet_articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
                
                # æå–æ–°æ¨æ–‡
                for article in tweet_articles:
                    if len(tweets_data) >= max_tweets:
                        break
                    
                    # extract_tweet_data ç°åœ¨ä¼šåœ¨å†…éƒ¨è¿›è¡Œç™½åå•è¿‡æ»¤
                    tweet_data = extract_tweet_data(article)
                    
                    if tweet_data and tweet_data['id'] not in processed_ids:
                        processed_ids.add(tweet_data['id'])
                        tweets_data.append(tweet_data)
                        
                        # æ ¹æ®æ¨æ–‡ç±»å‹æ˜¾ç¤ºä¸åŒçš„æ—¥å¿—
                        author = tweet_data['author']['handle_raw']
                        if tweet_data['is_retweet']:
                            logger.info(f"âœ“ æå–è½¬æ¨: @{tweet_data['retweeter']['handle_raw']} RT @{author} - {tweet_data['id']}")
                        elif tweet_data['is_quote']:
                            quoted_data = tweet_data.get('quoted')
                            quoted_author = quoted_data.get('author_handle', 'unknown') if quoted_data else 'unknown'
                            logger.info(f"âœ“ æå–å¼•ç”¨: @{author} å¼•ç”¨ @{quoted_author} - {tweet_data['id']}")
                        elif tweet_data['is_reply']:
                            reply_to = tweet_data.get('reply_to', [])
                            reply_str = ', '.join([f"@{u}" for u in reply_to]) if reply_to else '?'
                            logger.info(f"âœ“ æå–å›å¤: @{author} å›å¤ {reply_str} - {tweet_data['id']}")
                        else:
                            logger.info(f"âœ“ æå–åŸåˆ›: @{author} - {tweet_data['id']}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡
                if len(tweets_data) == before_scroll_count:
                    no_new_tweets_count += 1
                    logger.warning(f"æœªå‘ç°æ–°çš„ç¬¦åˆæ¡ä»¶çš„æ¨æ–‡ ({no_new_tweets_count}/{max_no_new_tweets})")
                else:
                    no_new_tweets_count = 0  # é‡ç½®è®¡æ•°å™¨
                    logger.info(f"å·²è·å– {len(tweets_data)}/{max_tweets} æ¡æ¨æ–‡")
        
        logger.info(f"âœ“ å…±æå– {len(tweets_data)} æ¡æ¨æ–‡")
        
        # å¦‚æœå¯ç”¨äº†ç”¨æˆ·è¿‡æ»¤ï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if ENABLE_USER_FILTER and MONITORED_USERS:
            logger.info(f"å·²å¯ç”¨ç”¨æˆ·ç™½åå•è¿‡æ»¤ï¼Œç›‘å¬ {len(MONITORED_USERS)} ä¸ªç”¨æˆ·")
        
        return tweets_data
        
    except Exception as e:
        logger.error(f"æŠ“å–åˆ—è¡¨å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return []

async def monitor_list_once(driver, list_url, pushed_ids):
    """ç›‘å¬åˆ—è¡¨ä¸€æ¬¡
    
    Args:
        driver: Selenium WebDriverå®ä¾‹
        list_url: æ¨ç‰¹åˆ—è¡¨URL
        pushed_ids: å·²æ¨é€çš„æ¨æ–‡IDé›†åˆ
        
    Returns:
        int: æ–°æ¨é€çš„æ¨æ–‡æ•°é‡
    """
    logger.info("=" * 60)
    logger.info(f"å¼€å§‹ç›‘å¬åˆ—è¡¨: {list_url}")
    logger.info("=" * 60)
    
    # æŠ“å–åˆ—è¡¨ä¸­çš„æ¨æ–‡ï¼ˆå¢å¼ºå¼‚å¸¸å¤„ç†ï¼‰
    tweets = []
    try:
        tweets = scrape_list_tweets(driver, list_url, MAX_TWEETS_PER_CHECK)
    except Exception as e:
        error_msg = f"æŠ“å–åˆ—è¡¨ {list_url} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"
        logger.error(error_msg)
        error_detail = traceback.format_exc()
        logger.error(error_detail)
        # å‘é€é”™è¯¯åˆ°webhook
        await send_error_to_webhook(error_msg, error_detail)
        # è¿”å›0ï¼Œè®©ä¸»å¾ªç¯ç»§ç»­
        return 0
    
    if not tweets:
        logger.info("æœªè·å–åˆ°æ¨æ–‡")
        return 0
    
    # è¿‡æ»¤å‡ºæ–°æ¨æ–‡ï¼ˆæœªæ¨é€è¿‡çš„ï¼‰
    new_tweets = [t for t in tweets if t['id'] not in pushed_ids]
    
    if not new_tweets:
        logger.info(f"æ²¡æœ‰æ–°æ¨æ–‡ï¼ˆå…±æ£€æŸ¥äº† {len(tweets)} æ¡æ¨æ–‡ï¼‰")
        return 0
    
    logger.info(f"å‘ç° {len(new_tweets)} æ¡æ–°æ¨æ–‡ï¼Œå‡†å¤‡æ¨é€...")
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæ—§çš„åœ¨å‰ï¼Œæ–°çš„åœ¨åï¼‰
    new_tweets.sort(key=lambda x: x['time'] if x['time'] else '')
    
    # æ¨é€æ–°æ¨æ–‡
    pushed_count = 0
    for idx, tweet in enumerate(new_tweets, 1):
        # ç®€åŒ–çš„æ—¥å¿—è¾“å‡º
        author = tweet['author']['handle_raw']
        if tweet['is_retweet']:
            logger.info(f"æ¨é€ {idx}/{len(new_tweets)}: è½¬æ¨ @{tweet['retweeter']['handle_raw']} RT @{author}")
        elif tweet['is_quote']:
            quoted_data = tweet.get('quoted')
            quoted_author = quoted_data.get('author_handle', 'unknown') if quoted_data else 'unknown'
            logger.info(f"æ¨é€ {idx}/{len(new_tweets)}: å¼•ç”¨ @{author} å¼•ç”¨ @{quoted_author}")
        elif tweet['is_reply']:
            reply_to = tweet.get('reply_to', [])
            reply_str = ', '.join([f"@{u}" for u in reply_to]) if reply_to else '?'
            logger.info(f"æ¨é€ {idx}/{len(new_tweets)}: å›å¤ @{author} å›å¤ {reply_str}")
        else:
            logger.info(f"æ¨é€ {idx}/{len(new_tweets)}: åŸåˆ› @{author}")
        
        # å‘é€åˆ°webhook
        success = await send_tweet_to_webhook(tweet)
        
        if success:
            # ä¿å­˜å·²æ¨é€ID
            save_pushed_id(tweet['id'])
            pushed_ids.add(tweet['id'])
            pushed_count += 1
        
        # æ¨æ–‡ä¹‹é—´é—´éš”
        if idx < len(new_tweets):
            await asyncio.sleep(2)
    
    logger.info(f"âœ“ æœ¬æ¬¡æˆåŠŸæ¨é€ {pushed_count} æ¡æ–°æ¨æ–‡")
    return pushed_count

def check_login_status(driver):
    """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
    try:
        driver.get("https://x.com/home")
        time.sleep(3)
        
        current_url = driver.current_url
        if "login" in current_url or "i/flow/login" in current_url:
            logger.warning("=" * 60)
            logger.warning("æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€")
            logger.warning("æµè§ˆå™¨å·²æ‰“å¼€ç™»å½•é¡µé¢ï¼Œè¯·åœ¨æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨ç™»å½•ä½ çš„Xè´¦å·ã€‚")
            logger.warning("ç™»å½•æˆåŠŸåï¼Œå›åˆ°è¿™é‡Œï¼ŒæŒ‰Enteré”®ç»§ç»­æ‰§è¡Œç›‘å¬...")
            logger.warning("æ³¨æ„ï¼šç™»å½•ä¿¡æ¯å°†ä¿å­˜åˆ°æœ¬åœ°ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶æ— éœ€é‡å¤ç™»å½•")
            logger.warning("=" * 60)
            input()
            return True
        else:
            logger.info("âœ“ æ£€æµ‹åˆ°å·²ç™»å½•çŠ¶æ€ï¼Œæ— éœ€é‡å¤ç™»å½•")
            return True
    except Exception as e:
        logger.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
        return False

async def monitor_lists_loop():
    """ä¸»ç›‘å¬å¾ªç¯"""
    logger.info("=" * 60)
    logger.info("æ¨ç‰¹åˆ—è¡¨ç›‘å¬è„šæœ¬å¯åŠ¨")
    logger.info("=" * 60)
    
    # åŠ è½½å·²æ¨é€çš„æ¨æ–‡ID
    pushed_ids = load_pushed_ids()
    logger.info(f"å·²æ¨é€çš„æ¨æ–‡æ•°: {len(pushed_ids)}")
    
    # å¯åŠ¨æµè§ˆå™¨
    driver = None
    try:
        driver = create_undetected_driver()
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not check_login_status(driver):
            logger.error("ç™»å½•æ£€æŸ¥å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            await send_error_to_webhook("ç™»å½•æ£€æŸ¥å¤±è´¥", "æ— æ³•ç™»å½•Twitterï¼Œç¨‹åºé€€å‡º")
            return
        
        logger.info(f"å¼€å§‹ç›‘å¬ {len(TWITTER_LISTS)} ä¸ªæ¨ç‰¹åˆ—è¡¨...")
        logger.info(f"æ£€æŸ¥é—´éš”: {LIST_CHECK_INTERVAL} ç§’")
        logger.info(f"æ¯æ¬¡æœ€å¤šè·å–: {MAX_TWEETS_PER_CHECK} æ¡æ¨æ–‡")
        logger.info("æç¤º: æŒ‰ Ctrl+C åœæ­¢ç›‘å¬")
        
        loop_count = 0
        # å¢åŠ ä¸€ä¸ªç™»å½•æ£€æŸ¥çš„è®¡æ•°å™¨
        login_check_interval_loops = 100  # æ¯100æ¬¡å¾ªç¯æ£€æŸ¥ä¸€æ¬¡ç™»å½•
        
        while True:
            loop_count += 1
            logger.info(f"ç¬¬ {loop_count} æ¬¡æ£€æŸ¥")
            
            # å®šæœŸæ£€æŸ¥ç™»å½•çŠ¶æ€
            if loop_count % login_check_interval_loops == 0:
                logger.info("å®šæœŸæ£€æŸ¥ç™»å½•çŠ¶æ€...")
                check_login_status(driver)
            
            total_new_tweets = 0
            
            # éå†æ‰€æœ‰åˆ—è¡¨
            for list_url in TWITTER_LISTS:
                try:
                    new_count = await monitor_list_once(driver, list_url, pushed_ids)
                    total_new_tweets += new_count
                    
                    # åˆ—è¡¨ä¹‹é—´çš„é—´éš”
                    if list_url != TWITTER_LISTS[-1]:
                        random_sleep(3, 6)
                        
                except Exception as e:
                    error_msg = f"ç›‘å¬åˆ—è¡¨å‡ºé”™: {e}"
                    logger.error(error_msg)
                    error_detail = traceback.format_exc()
                    logger.error(error_detail)
                    await send_error_to_webhook(error_msg, error_detail)
                    continue
            
            # æ‰“å°æœ¬è½®æ€»ç»“
            logger.info("=" * 60)
            logger.info(f"ç¬¬ {loop_count} æ¬¡æ£€æŸ¥å®Œæˆ")
            logger.info(f"æœ¬è½®å…±æ¨é€ {total_new_tweets} æ¡æ–°æ¨æ–‡")
            logger.info(f"ä¸‹æ¬¡æ£€æŸ¥æ—¶é—´: {LIST_CHECK_INTERVAL} ç§’å")
            logger.info("=" * 60)
            
            # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
            time.sleep(LIST_CHECK_INTERVAL)
            
    except KeyboardInterrupt:
        logger.info("\næ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
        
    except Exception as e:
        error_msg = f"ç›‘å¬è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"
        logger.critical(error_msg)
        error_detail = traceback.format_exc()
        logger.critical(error_detail)
        await send_error_to_webhook(error_msg, error_detail)
        
    finally:
        if driver:
            logger.info("å…³é—­æµè§ˆå™¨...")
            driver.quit()
        logger.info("ç›‘å¬å·²åœæ­¢")

# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == "__main__":
    # è¿è¡Œç›‘å¬å¾ªç¯
    asyncio.run(monitor_lists_loop())

