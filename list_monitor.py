#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨ç‰¹åˆ—è¡¨ç›‘å¬è„šæœ¬
ç›‘å¬æŒ‡å®šæ¨ç‰¹åˆ—è¡¨ä¸­çš„æ–°æ¨æ–‡ï¼Œå¹¶é€šè¿‡webhookæ¨é€
"""

import re
import time
import os
import requests
import random
import json
import asyncio
import base64
import logging
import sys
import traceback
from datetime import datetime
from io import BytesIO
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException

# ==================== æ—¥å¿—é…ç½® ====================
class ColoredFormatter(logging.Formatter):
    """å½©è‰²æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    COLORS = {
        'DEBUG': '\033[36m',    # é’è‰²
        'INFO': '\033[32m',     # ç»¿è‰²
        'WARNING': '\033[33m',  # é»„è‰²
        'ERROR': '\033[31m',    # çº¢è‰²
        'CRITICAL': '\033[35m', # ç´«è‰²
        'RESET': '\033[0m'      # é‡ç½®
    }
    
    def format(self, record):
        # æ·»åŠ é¢œè‰²
        levelname = record.levelname
        if levelname in self.COLORS:
            record.levelname = f"{self.COLORS[levelname]}{levelname}{self.COLORS['RESET']}"
        return super().format(record)

def setup_logger():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logger = logging.getLogger('ListMonitor')
    logger.setLevel(logging.DEBUG)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG)
    
    # å½©è‰²æ ¼å¼
    formatter = ColoredFormatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(formatter)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆä¿å­˜æ‰€æœ‰æ—¥å¿—ï¼‰
    file_handler = logging.FileHandler('list_monitor.log', encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

# åˆå§‹åŒ–logger
logger = setup_logger()

# å¯¼å…¥é…ç½®å’Œwebhookæ¨¡å—
try:
    from config import (
        CHROME_PROXY, USE_PERSISTENT_PROFILE, PROFILE_DIR,
        PUSHED_IDS_FILE, LIST_CHECK_INTERVAL, MAX_TWEETS_PER_CHECK,
        TWITTER_LISTS, MONITORED_USERS, ENABLE_USER_FILTER
    )
    from webhook import send_message_async, send_image_async
except ImportError as e:
    logger.critical(f"å¯¼å…¥é…ç½®å¤±è´¥: {e}")
    logger.critical("è¯·ç¡®ä¿ config.py å’Œ webhook.py æ–‡ä»¶å­˜åœ¨")
    exit(1)

# ==================== å¼‚å¸¸å¤„ç† ====================
async def send_error_to_webhook(error_msg, error_detail=""):
    """å‘é€é”™è¯¯ä¿¡æ¯åˆ°webhook
    
    Args:
        error_msg: é”™è¯¯æ¶ˆæ¯
        error_detail: é”™è¯¯è¯¦æƒ…
    """
    try:
        message_parts = [
            "âš ï¸ åˆ—è¡¨ç›‘å¬ç¨‹åºå¼‚å¸¸",
            f"{'='*50}",
            f"âŒ é”™è¯¯: {error_msg}",
        ]
        
        if error_detail:
            message_parts.append(f"\nğŸ“‹ è¯¦æƒ…:\n{error_detail}")
        
        message_parts.append(f"\nâ° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        message = "\n".join(message_parts)
        await send_message_async(message, msg_type="text")
        logger.info("é”™è¯¯ä¿¡æ¯å·²æ¨é€åˆ°webhook")
    except Exception as e:
        logger.error(f"æ¨é€é”™è¯¯ä¿¡æ¯å¤±è´¥: {e}")

# ==================== è¾…åŠ©å‡½æ•° ====================
def load_pushed_ids():
    """åŠ è½½å·²æ¨é€çš„æ¨æ–‡ID"""
    if os.path.exists(PUSHED_IDS_FILE):
        try:
            with open(PUSHED_IDS_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æ¨é€IDå¤±è´¥: {e}")
            return set()
    return set()

def save_pushed_id(tweet_id):
    """ä¿å­˜å·²æ¨é€çš„æ¨æ–‡ID"""
    pushed_ids = load_pushed_ids()
    pushed_ids.add(tweet_id)
    with open(PUSHED_IDS_FILE, 'w', encoding='utf-8') as f:
        json.dump(list(pushed_ids), f, indent=2)

def random_sleep(min_sec=2, max_sec=5):
    """éšæœºå»¶æ—¶ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
    sleep_time = random.uniform(min_sec, max_sec)
    logger.debug(f"éšæœºç­‰å¾… {sleep_time:.2f} ç§’...")
    time.sleep(sleep_time)

def create_undetected_driver():
    """åˆ›å»ºåæ£€æµ‹çš„Chromeæµè§ˆå™¨å®ä¾‹"""
    logger.info("æ­£åœ¨å¯åŠ¨ undetected_chromedriver...")
    
    options = uc.ChromeOptions()
    
    # æ·»åŠ ä»£ç†é…ç½®
    if CHROME_PROXY:
        logger.info(f"é…ç½®ä»£ç†: {CHROME_PROXY}")
        options.add_argument(f'--proxy-server={CHROME_PROXY}')
    
    # ä½¿ç”¨æŒä¹…åŒ–Profile
    if USE_PERSISTENT_PROFILE:
        logger.info(f"ä½¿ç”¨æŒä¹…åŒ–Profile: {PROFILE_DIR}")
        options.add_argument(f'--user-data-dir={PROFILE_DIR}')
    
    # å…¶ä»–ä¼˜åŒ–é€‰é¡¹
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    try:
        driver = uc.Chrome(options=options)
        logger.info("âœ“ undetected_chromedriver å¯åŠ¨æˆåŠŸ")
        return driver
    except Exception as e:
        logger.error(f"å¯åŠ¨å¤±è´¥: {e}")
        raise

def download_image_to_base64(url, proxy=None):
    """ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64ç¼–ç 
    
    Args:
        url: å›¾ç‰‡URL
        proxy: ä»£ç†é…ç½®
    
    Returns:
        str: base64ç¼–ç çš„å›¾ç‰‡ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        proxies = {'http': proxy, 'https': proxy} if proxy else None
        
        response = requests.get(url, headers=headers, proxies=proxies, timeout=15)
        if response.status_code == 200:
            # å°†å›¾ç‰‡æ•°æ®è½¬æ¢ä¸ºbase64
            image_base64 = base64.b64encode(response.content).decode('utf-8')
            return image_base64
        else:
            logger.warning(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            return None
    except Exception as e:
        logger.error(f"ä¸‹è½½å›¾ç‰‡å¼‚å¸¸: {e}")
        return None

def extract_tweet_data(tweet_article):
    """ä»æ¨æ–‡å…ƒç´ ä¸­æå–æ•°æ®ï¼Œèƒ½å¤ŸåŒºåˆ†åŸåˆ›å’Œè½¬æ¨
    
    Args:
        tweet_article: Selenium WebElementï¼Œæ¨æ–‡çš„articleå…ƒç´ 
        
    Returns:
        dict: åŒ…å«æ¨æ–‡æ•°æ®çš„å­—å…¸ï¼Œå¦‚æœä¸ç¬¦åˆç™½åå•è¦æ±‚åˆ™è¿”å›None
    """
    try:
        is_retweet = False
        retweeter_handle_raw = ""
        retweeter_display_name = ""
        
        # è·å–æ¨æ–‡çš„åŸå§‹HTMLç”¨äºè°ƒè¯• (ä»…åœ¨éœ€è¦æ—¶å¯ç”¨)
        # tweet_html = tweet_article.get_attribute('outerHTML')
        # logger.debug(f"æ¨æ–‡HTMLç‰‡æ®µ: {tweet_html[:200]}...")
        
        # æ£€æµ‹æ˜¯å¦ä¸ºè½¬æ¨ - å¤šç§ç­–ç•¥
        try:
            # ç­–ç•¥1: æ£€æŸ¥socialContext (æœ€å¸¸è§)
            social_context = tweet_article.find_element(By.XPATH, ".//div[@data-testid='socialContext']")
            context_text = social_context.text
            logger.debug(f"æ‰¾åˆ°socialContextï¼Œå†…å®¹: {context_text}")
            
            # è½¬æ¨å…³é”®è¯åˆ—è¡¨ (æ”¯æŒå¤šè¯­è¨€)
            retweets_keywords = [
                "Retweeted", "è½¬æ¨äº†", "å·²è½¬æ¨", "å·²è½¬å¸–", 
                "reposted", "è½¬å‘äº†", "retweet", "Retweet",
                "Reposted"  # X çš„æ–°æœ¯è¯­
            ]
            matched_keyword = None
            
            for keyword in retweets_keywords:
                if keyword.lower() in context_text.lower():  # ä¸åŒºåˆ†å¤§å°å†™
                    matched_keyword = keyword
                    is_retweet = True
                    logger.debug(f"âœ“ åŒ¹é…åˆ°è½¬æ¨å…³é”®è¯: {keyword}")
                    break
            
            if is_retweet:
                # æå–è½¬æ¨è€…çš„ç”¨æˆ·å
                try:
                    # æ–¹æ³•1: ä»socialContextä¸­çš„é“¾æ¥æå– (æœ€å¯é )
                    retweeter_links = social_context.find_elements(By.TAG_NAME, "a")
                    for link in retweeter_links:
                        href = link.get_attribute('href')
                        if href and '/status/' not in href and '/photo/' not in href:
                            # æå–ç”¨æˆ·å
                            retweeter_handle_raw = href.split('/')[-1].split('?')[0].split('#')[0]
                            if retweeter_handle_raw:
                                break
                    
                    # æ–¹æ³•2: æå–æ˜¾ç¤ºåç§°
                    # æ¸…ç†æ˜¾ç¤ºåç§°ï¼Œç§»é™¤"Retweeted"ç­‰æ–‡å­—å’Œå¤šä½™ç©ºæ ¼
                    retweeter_display_name = context_text
                    if matched_keyword:
                        retweeter_display_name = retweeter_display_name.replace(matched_keyword, "").strip()
                    # ç§»é™¤å…¶ä»–å¯èƒ½çš„å¹²æ‰°æ–‡æœ¬
                    retweeter_display_name = re.sub(r'\s+', ' ', retweeter_display_name).strip()
                    
                    # æ–¹æ³•3: å¦‚æœä¸Šé¢çš„æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ä»spanä¸­æå–
                    if not retweeter_display_name:
                        try:
                            spans = social_context.find_elements(By.TAG_NAME, "span")
                            for span in spans:
                                text = span.text.strip()
                                if text and text not in retweets_keywords:
                                    retweeter_display_name = text
                                    break
                        except:
                            pass
                    
                    logger.debug(f"æ£€æµ‹åˆ°è½¬æ¨ - è½¬æ¨è€…: @{retweeter_handle_raw} ({retweeter_display_name})")
                except Exception as e:
                    logger.debug(f"æå–è½¬æ¨è€…ä¿¡æ¯å¤±è´¥: {e}")
                    
        except NoSuchElementException:
            # logger.debug("æœªæ‰¾åˆ°socialContextå…ƒç´ ")
            # ç­–ç•¥2: æ£€æŸ¥æ˜¯å¦æœ‰è½¬æ¨å›¾æ ‡ (backup)
            try:
                retweet_icon = tweet_article.find_element(By.XPATH, ".//div[@data-testid='retweet']")
                # å¦‚æœæ‰¾åˆ°äº†è½¬æ¨å›¾æ ‡ï¼Œè¯´æ˜å¯èƒ½æ˜¯è½¬æ¨
                # ä½†è¿™ä¸ªä¸å¤ªå¯é ï¼Œä»…ä½œä¸ºå¤‡ç”¨æ£€æµ‹
                #logger.debug("æ‰¾åˆ°retweetå›¾æ ‡ï¼Œä½†æ— socialContextï¼Œå¯èƒ½æ˜¯ç‰¹æ®Šæƒ…å†µ")
                pass
            except NoSuchElementException:
                #logger.debug("ä¹Ÿæœªæ‰¾åˆ°retweetå›¾æ ‡")
                pass
            
            # æ‰¾ä¸åˆ°socialContextï¼Œè¯´æ˜å¾ˆå¯èƒ½æ˜¯åŸåˆ›æ¨æ–‡
            is_retweet = False
            # logger.debug("åˆ¤å®šä¸ºåŸåˆ›æ¨æ–‡")
        
        # æå–ç”¨æˆ·åå’Œæ˜¾ç¤ºåç§°ï¼ˆè¿™æ˜¯åŸä½œè€…çš„ä¿¡æ¯ï¼‰
        user_handle_raw = ""
        user_display_name = ""
        try:
            user_name_div = tweet_article.find_element(By.XPATH, ".//div[@data-testid='User-Name']")
            
            # æå–handle (@username) - ä¼˜å…ˆçº§æ›´é«˜ï¼Œå› ä¸ºæ›´å‡†ç¡®
            try:
                # å°è¯•å¤šç§æ–¹æ³•æå–ç”¨æˆ·å
                handle_links = user_name_div.find_elements(By.XPATH, ".//a[contains(@href, '/')]")
                for link in handle_links:
                    href = link.get_attribute('href')
                    if href and '/status/' not in href and '/photo/' not in href:
                        # æå–ç”¨æˆ·åï¼Œç¡®ä¿æ¸…ç†æ‰€æœ‰å¤šä½™å‚æ•°
                        user_handle_raw = href.split('/')[-1].split('?')[0].split('#')[0]
                        if user_handle_raw:  # æ‰¾åˆ°äº†å°±é€€å‡º
                            break
                
                logger.debug(f"æå–åˆ°ç”¨æˆ·å: @{user_handle_raw}")
            except Exception as e:
                logger.debug(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
            
            # æå–æ˜¾ç¤ºåç§°
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨
                display_name_element = None
                
                # æ–¹æ³•1: é€šè¿‡CSSç±»å
                try:
                    display_name_element = user_name_div.find_element(By.XPATH, ".//span[contains(@class, 'css-1jxf684')]")
                except:
                    pass
                
                # æ–¹æ³•2: é€šè¿‡ç»“æ„æŸ¥æ‰¾ (backup)
                if not display_name_element:
                    try:
                        spans = user_name_div.find_elements(By.TAG_NAME, "span")
                        for span in spans:
                            text = span.text.strip()
                            # æ˜¾ç¤ºåç§°é€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªé@å¼€å¤´çš„æ–‡æœ¬
                            if text and not text.startswith('@') and len(text) > 0:
                                display_name_element = span
                                break
                    except:
                        pass
                
                if display_name_element:
                    user_display_name = display_name_element.text.strip()
                    logger.debug(f"æå–åˆ°æ˜¾ç¤ºåç§°: {user_display_name}")
                    
            except Exception as e:
                logger.debug(f"æå–æ˜¾ç¤ºåç§°å¤±è´¥: {e}")
                
        except NoSuchElementException:
            #logger.debug("æœªæ‰¾åˆ°User-Nameå…ƒç´ ")
            pass
        
        # ç™½åå•è¿‡æ»¤é€»è¾‘
        # å¦‚æœæ˜¯è½¬æ¨ï¼Œæ£€æŸ¥è½¬æ¨è€…æ˜¯å¦åœ¨ç™½åå•
        if is_retweet:
            # logger.debug(f"è½¬æ¨æ£€æµ‹ - è½¬æ¨è€…: @{retweeter_handle_raw}, åŸä½œè€…: @{user_handle_raw}")
            if not is_user_in_whitelist(retweeter_handle_raw):
                # logger.debug(f"è·³è¿‡è½¬æ¨ - è½¬æ¨è€… @{retweeter_handle_raw} ä¸åœ¨ç™½åå•")
                return None  # è½¬æ¨è€…ä¸åœ¨ç™½åå•ï¼Œè·³è¿‡
            # logger.debug(f"âœ“ è½¬æ¨è€… @{retweeter_handle_raw} åœ¨ç™½åå•ä¸­")
        # å¦‚æœæ˜¯åŸåˆ›ï¼Œæ£€æŸ¥ä½œè€…æ˜¯å¦åœ¨ç™½åå•
        else:
            # logger.debug(f"åŸåˆ›æ¨æ–‡æ£€æµ‹ - ä½œè€…: @{user_handle_raw}")
            if not is_user_in_whitelist(user_handle_raw):
                # logger.debug(f"è·³è¿‡åŸåˆ› - ä½œè€… @{user_handle_raw} ä¸åœ¨ç™½åå•")
                return None  # ä½œè€…ä¸åœ¨ç™½åå•ï¼Œè·³è¿‡
            # logger.debug(f"âœ“ ä½œè€… @{user_handle_raw} åœ¨ç™½åå•ä¸­")
        
        # åªæœ‰é€šè¿‡ç™½åå•æ£€æŸ¥çš„æ¨æ–‡æ‰ä¼šç»§ç»­æå–å…¶ä»–ä¿¡æ¯
        
        # éªŒè¯æå–çš„ç”¨æˆ·ä¿¡æ¯
        if is_retweet:
            if not retweeter_handle_raw:
                logger.warning("è½¬æ¨æ£€æµ‹æˆåŠŸï¼Œä½†æœªèƒ½æå–åˆ°è½¬æ¨è€…ç”¨æˆ·åï¼Œå¯èƒ½å­˜åœ¨è§£æé—®é¢˜")
            if not user_handle_raw:
                logger.warning("è½¬æ¨æ£€æµ‹æˆåŠŸï¼Œä½†æœªèƒ½æå–åˆ°åŸä½œè€…ç”¨æˆ·åï¼Œå¯èƒ½å­˜åœ¨è§£æé—®é¢˜")
        else:
            if not user_handle_raw:
                logger.warning("åŸåˆ›æ¨æ–‡æœªèƒ½æå–åˆ°ä½œè€…ç”¨æˆ·åï¼Œå¯èƒ½å­˜åœ¨è§£æé—®é¢˜")
        
        # æå–æ¨æ–‡IDå’ŒURL
        tweet_link_elements = tweet_article.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
        tweet_url = None
        tweet_id = None
        
        for link in tweet_link_elements:
            href = link.get_attribute('href')
            if '/status/' in href and '/analytics' not in href:  # æ’é™¤analyticsé“¾æ¥
                tweet_url = href
                # æ¸…ç†URLï¼Œç§»é™¤é¢å¤–çš„è·¯å¾„
                tweet_id = href.split('/status/')[-1].split('?')[0].split('/')[0]
                break
        
        if not tweet_id:
            return None
        
        # æå–æ¨æ–‡æ­£æ–‡
        tweet_text = ""
        try:
            tweet_text_element = tweet_article.find_element(By.XPATH, ".//div[@data-testid='tweetText']")
            tweet_text = tweet_text_element.text
        except NoSuchElementException:
            pass
        
        # æå–æ¨æ–‡æ—¶é—´
        tweet_time = ""
        try:
            time_element = tweet_article.find_element(By.XPATH, ".//time")
            tweet_time = time_element.get_attribute('datetime')
            # æ ¼å¼åŒ–æ—¶é—´ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
            if tweet_time:
                dt = datetime.fromisoformat(tweet_time.replace('Z', '+00:00'))
                tweet_time = dt.strftime('%Y-%m-%d %H:%M:%S')
        except NoSuchElementException:
            pass
        
        # æå–å›¾ç‰‡URL
        image_urls = []
        try:
            photo_divs = tweet_article.find_elements(By.XPATH, ".//div[@data-testid='tweetPhoto']")
            
            for photo_div in photo_divs:
                img_elements = photo_div.find_elements(By.TAG_NAME, "img")
                for img_element in img_elements:
                    img_url = img_element.get_attribute('src')
                    
                    if img_url:
                        # å°†URLå‚æ•°æ›¿æ¢ä¸º 'name=orig' æ¥è·å–æœ€é«˜æ¸…çš„åŸå›¾
                        if 'name=' in img_url:
                            orig_img_url = re.sub(r'name=\w+', 'name=orig', img_url)
                        else:
                            orig_img_url = img_url.split('?')[0] + '?format=jpg&name=orig'
                        
                        if orig_img_url not in image_urls:
                            image_urls.append(orig_img_url)
        except:
            pass
        
        # æ„å»ºè¿”å›æ•°æ®
        tweet_data = {
            "id": tweet_id,
            "url": tweet_url,
            "is_retweet": is_retweet,
            "retweeter": {  # è½¬æ¨è€…ä¿¡æ¯
                "handle_raw": retweeter_handle_raw,
                "display_name": retweeter_display_name
            },
            "original_author": {  # åŸä½œè€…ä¿¡æ¯
                "handle_raw": user_handle_raw,
                "display_name": user_display_name
            },
            "time": tweet_time,
            "text": tweet_text,
            "images": image_urls
        }
        
        # è¾“å‡ºæå–æˆåŠŸçš„æ‘˜è¦ä¿¡æ¯
        if is_retweet:
            logger.debug(f"âœ“ æˆåŠŸæå–è½¬æ¨æ•°æ® - ID: {tweet_id}, è½¬æ¨è€…: @{retweeter_handle_raw}, åŸä½œè€…: @{user_handle_raw}")
        else:
            logger.debug(f"âœ“ æˆåŠŸæå–åŸåˆ›æ•°æ® - ID: {tweet_id}, ä½œè€…: @{user_handle_raw}")
        
        return tweet_data
        
    except Exception as e:
        logger.debug(f"æå–æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
        logger.debug(traceback.format_exc())
        return None

async def send_tweet_to_webhook(tweet_data):
    """å°†æ¨æ–‡æ•°æ®å‘é€åˆ°webhookï¼Œä¸ºè½¬æ¨æä¾›ä¸“é—¨æ ¼å¼
    
    Args:
        tweet_data: æ¨æ–‡æ•°æ®å­—å…¸
    """
    try:
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        message_parts = []
        
        # æ ¹æ®æ˜¯å¦ä¸ºè½¬æ¨ï¼Œç”Ÿæˆä¸åŒæ ¼å¼çš„æ¶ˆæ¯
        if tweet_data.get('is_retweet'):
            # è¿™æ˜¯è½¬æ¨
            message_parts.append(f"ğŸ”„ ç”¨æˆ·è½¬æ¨æé†’")
            retweeter = tweet_data['retweeter']
            original_author = tweet_data['original_author']
            
            # çªå‡ºè½¬æ¨è€…
            if retweeter['display_name']:
                message_parts.append(f"ğŸ‘¤ è½¬æ¨è€…: {retweeter['display_name']} (@{retweeter['handle_raw']})")
            else:
                message_parts.append(f"ğŸ‘¤ è½¬æ¨è€…: @{retweeter['handle_raw']}")
            
            # æ ‡æ˜åŸä½œè€…
            if original_author['display_name']:
                message_parts.append(f"âœï¸ åŸä½œè€…: {original_author['display_name']} (@{original_author['handle_raw']})")
            else:
                message_parts.append(f"âœï¸ åŸä½œè€…: @{original_author['handle_raw']}")
        else:
            # è¿™æ˜¯åŸåˆ›æ¨æ–‡
            message_parts.append(f"ğŸ¦ æ–°æ¨æ–‡ç›‘å¬")
            author = tweet_data['original_author']  # å¯¹äºåŸåˆ›ï¼Œoriginal_authorå°±æ˜¯ä½œè€…
            
            if author['display_name']:
                message_parts.append(f"ğŸ‘¤ ä½œè€…: {author['display_name']} (@{author['handle_raw']})")
            else:
                message_parts.append(f"ğŸ‘¤ ä½œè€…: @{author['handle_raw']}")
        
        # æ—¶é—´
        if tweet_data['time']:
            message_parts.append(f"ğŸ• æ—¶é—´: {tweet_data['time']}")
        
        message_parts.append(f"")
        
        # æ¨æ–‡å†…å®¹
        if tweet_data['text']:
            message_parts.append(f"ğŸ“ å†…å®¹:")
            message_parts.append(tweet_data['text'])
            message_parts.append(f"")
        
        # æ¨æ–‡é“¾æ¥
        if tweet_data['url']:
            message_parts.append(f"ğŸ”— é“¾æ¥: {tweet_data['url']}")
        
        # å›¾ç‰‡ä¿¡æ¯
        if tweet_data['images']:
            message_parts.append(f"ğŸ–¼ï¸ åŒ…å« {len(tweet_data['images'])} å¼ å›¾ç‰‡")
        
        # å‘é€æ–‡æœ¬æ¶ˆæ¯
        message = "\n".join(message_parts)
        
        # å‘é€å‰å…ˆè¾“å‡ºæ¨æ–‡å†…å®¹åˆ°æ—¥å¿—
        logger.info("=" * 60)
        logger.info("å‡†å¤‡å‘é€æ¨æ–‡åˆ°webhook:")
        logger.info("-" * 60)
        for line in message.split('\n'):
            logger.info(line)
        logger.info("=" * 60)
        
        await send_message_async(message, msg_type="text")
        
        # å‘é€å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if tweet_data['images']:
            logger.info(f"å‡†å¤‡å‘é€ {len(tweet_data['images'])} å¼ å›¾ç‰‡...")
            for idx, img_url in enumerate(tweet_data['images'], 1):
                logger.info(f"ä¸‹è½½å›¾ç‰‡ {idx}/{len(tweet_data['images'])}...")
                
                # ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
                proxy = CHROME_PROXY if CHROME_PROXY else None
                image_base64 = download_image_to_base64(img_url, proxy)
                
                if image_base64:
                    logger.info(f"å‘é€å›¾ç‰‡ {idx}/{len(tweet_data['images'])}...")
                    success = await send_image_async(image_base64=image_base64)
                    if success:
                        logger.info(f"âœ“ å›¾ç‰‡ {idx} å‘é€æˆåŠŸ")
                    else:
                        logger.warning(f"å›¾ç‰‡ {idx} å‘é€å¤±è´¥")
                    
                    # å›¾ç‰‡å‘é€é—´éš”
                    if idx < len(tweet_data['images']):
                        await asyncio.sleep(1)
                else:
                    logger.warning(f"å›¾ç‰‡ {idx} ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡")
        
        logger.info(f"âœ“ æ¨æ–‡æ¨é€å®Œæˆ: {tweet_data['id']}")
        return True
        
    except Exception as e:
        logger.error(f"æ¨é€æ¨æ–‡å¤±è´¥: {e}")
        error_detail = traceback.format_exc()
        await send_error_to_webhook("æ¨é€æ¨æ–‡åˆ°webhookå¤±è´¥", error_detail)
        return False

def is_user_in_whitelist(user_handle_raw):
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨ç™½åå•ä¸­
    
    Args:
        user_handle_raw: ç”¨æˆ·åï¼ˆä¸å«@ï¼‰
        
    Returns:
        bool: å¦‚æœç”¨æˆ·åœ¨ç™½åå•æˆ–æœªå¯ç”¨è¿‡æ»¤ï¼Œè¿”å›True
    """
    # å¦‚æœæœªå¯ç”¨ç”¨æˆ·è¿‡æ»¤ï¼Œè¿”å›Trueï¼ˆå…è®¸æ‰€æœ‰ç”¨æˆ·ï¼‰
    if not ENABLE_USER_FILTER:
        return True
    
    # å¦‚æœç™½åå•ä¸ºç©ºï¼Œè¿”å›Trueï¼ˆå…è®¸æ‰€æœ‰ç”¨æˆ·ï¼‰
    if not MONITORED_USERS:
        return True
    
    # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨ç™½åå•ä¸­ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    return user_handle_raw.lower() in [u.lower() for u in MONITORED_USERS]

def scrape_list_tweets(driver, list_url, max_tweets=20):
    """æŠ“å–æ¨ç‰¹åˆ—è¡¨ä¸­çš„æ¨æ–‡
    
    Args:
        driver: Selenium WebDriverå®ä¾‹
        list_url: æ¨ç‰¹åˆ—è¡¨URL
        max_tweets: æœ€å¤šæŠ“å–çš„æ¨æ–‡æ•°é‡
        
    Returns:
        list: æ¨æ–‡æ•°æ®åˆ—è¡¨
    """
    logger.info(f"è®¿é—®åˆ—è¡¨: {list_url}")
    
    try:
        driver.get(list_url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        wait = WebDriverWait(driver, 15)
        
        # ç­‰å¾…æ¨æ–‡åŠ è½½
        logger.info("ç­‰å¾…æ¨æ–‡åŠ è½½...")
        time.sleep(5)
        
        tweets_data = []
        processed_ids = set()
        no_new_tweets_count = 0  # è¿ç»­æœªå‘ç°æ–°æ¨æ–‡çš„æ¬¡æ•°
        max_no_new_tweets = 3     # æœ€å¤šå…è®¸3æ¬¡æœªå‘ç°æ–°æ¨æ–‡
        
        # å…ˆè·å–å½“å‰å¯è§çš„æ¨æ–‡ï¼ˆä¸æ»šåŠ¨ï¼‰
        logger.info("æŠ“å–å½“å‰å¯è§æ¨æ–‡...")
        tweet_articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
        logger.info(f"æ‰¾åˆ° {len(tweet_articles)} æ¡å¯è§æ¨æ–‡")
        
        for article in tweet_articles:
            if len(tweets_data) >= max_tweets:
                break
            
            # extract_tweet_data ç°åœ¨ä¼šåœ¨å†…éƒ¨è¿›è¡Œç™½åå•è¿‡æ»¤ï¼Œè¿”å›Noneè¡¨ç¤ºä¸ç¬¦åˆè¦æ±‚
            tweet_data = extract_tweet_data(article)
            
            if tweet_data and tweet_data['id'] not in processed_ids:
                processed_ids.add(tweet_data['id'])
                tweets_data.append(tweet_data)
                
                # æ ¹æ®æ˜¯å¦ä¸ºè½¬æ¨æ˜¾ç¤ºä¸åŒçš„æ—¥å¿—
                if tweet_data['is_retweet']:
                    logger.info(f"âœ“ æå–è½¬æ¨: @{tweet_data['retweeter']['handle_raw']} RT @{tweet_data['original_author']['handle_raw']} - {tweet_data['id']}")
                else:
                    logger.info(f"âœ“ æå–åŸåˆ›: @{tweet_data['original_author']['handle_raw']} - {tweet_data['id']}")
        
        # å¦‚æœéœ€è¦æ›´å¤šæ¨æ–‡ï¼Œæ‰è¿›è¡Œæ»šåŠ¨
        if len(tweets_data) < max_tweets:
            logger.info(f"å½“å‰è·å– {len(tweets_data)} æ¡ï¼Œéœ€è¦æ›´å¤šæ¨æ–‡ï¼Œå¼€å§‹æ»šåŠ¨...")
            
            while len(tweets_data) < max_tweets and no_new_tweets_count < max_no_new_tweets:
                # è®°å½•æ»šåŠ¨å‰çš„æ¨æ–‡æ•°é‡
                before_scroll_count = len(tweets_data)
                
                # æ»šåŠ¨é¡µé¢
                driver.execute_script("window.scrollBy(0, 800);")  # æ¯æ¬¡åªæ»šåŠ¨800åƒç´ ï¼Œé¿å…è·³è¿‡å†…å®¹
                time.sleep(2)
                
                # è·å–æ–°å‡ºç°çš„æ¨æ–‡
                tweet_articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
                
                # æå–æ–°æ¨æ–‡
                for article in tweet_articles:
                    if len(tweets_data) >= max_tweets:
                        break
                    
                    # extract_tweet_data ç°åœ¨ä¼šåœ¨å†…éƒ¨è¿›è¡Œç™½åå•è¿‡æ»¤
                    tweet_data = extract_tweet_data(article)
                    
                    if tweet_data and tweet_data['id'] not in processed_ids:
                        processed_ids.add(tweet_data['id'])
                        tweets_data.append(tweet_data)
                        
                        # æ ¹æ®æ˜¯å¦ä¸ºè½¬æ¨æ˜¾ç¤ºä¸åŒçš„æ—¥å¿—
                        if tweet_data['is_retweet']:
                            logger.info(f"âœ“ æå–è½¬æ¨: @{tweet_data['retweeter']['handle_raw']} RT @{tweet_data['original_author']['handle_raw']} - {tweet_data['id']}")
                        else:
                            logger.info(f"âœ“ æå–åŸåˆ›: @{tweet_data['original_author']['handle_raw']} - {tweet_data['id']}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡
                if len(tweets_data) == before_scroll_count:
                    no_new_tweets_count += 1
                    logger.warning(f"æœªå‘ç°æ–°çš„ç¬¦åˆæ¡ä»¶çš„æ¨æ–‡ ({no_new_tweets_count}/{max_no_new_tweets})")
                else:
                    no_new_tweets_count = 0  # é‡ç½®è®¡æ•°å™¨
                    logger.info(f"å·²è·å– {len(tweets_data)}/{max_tweets} æ¡æ¨æ–‡")
        
        logger.info(f"âœ“ å…±æå– {len(tweets_data)} æ¡æ¨æ–‡")
        
        # å¦‚æœå¯ç”¨äº†ç”¨æˆ·è¿‡æ»¤ï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if ENABLE_USER_FILTER and MONITORED_USERS:
            logger.info(f"å·²å¯ç”¨ç”¨æˆ·ç™½åå•è¿‡æ»¤ï¼Œç›‘å¬ {len(MONITORED_USERS)} ä¸ªç”¨æˆ·")
        
        return tweets_data
        
    except Exception as e:
        logger.error(f"æŠ“å–åˆ—è¡¨å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return []

async def monitor_list_once(driver, list_url, pushed_ids):
    """ç›‘å¬åˆ—è¡¨ä¸€æ¬¡
    
    Args:
        driver: Selenium WebDriverå®ä¾‹
        list_url: æ¨ç‰¹åˆ—è¡¨URL
        pushed_ids: å·²æ¨é€çš„æ¨æ–‡IDé›†åˆ
        
    Returns:
        int: æ–°æ¨é€çš„æ¨æ–‡æ•°é‡
    """
    logger.info("=" * 60)
    logger.info(f"å¼€å§‹ç›‘å¬åˆ—è¡¨: {list_url}")
    logger.info("=" * 60)
    
    # æŠ“å–åˆ—è¡¨ä¸­çš„æ¨æ–‡ï¼ˆå¢å¼ºå¼‚å¸¸å¤„ç†ï¼‰
    tweets = []
    try:
        tweets = scrape_list_tweets(driver, list_url, MAX_TWEETS_PER_CHECK)
    except Exception as e:
        error_msg = f"æŠ“å–åˆ—è¡¨ {list_url} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"
        logger.error(error_msg)
        error_detail = traceback.format_exc()
        logger.error(error_detail)
        # å‘é€é”™è¯¯åˆ°webhook
        await send_error_to_webhook(error_msg, error_detail)
        # è¿”å›0ï¼Œè®©ä¸»å¾ªç¯ç»§ç»­
        return 0
    
    if not tweets:
        logger.info("æœªè·å–åˆ°æ¨æ–‡")
        return 0
    
    # è¿‡æ»¤å‡ºæ–°æ¨æ–‡ï¼ˆæœªæ¨é€è¿‡çš„ï¼‰
    new_tweets = [t for t in tweets if t['id'] not in pushed_ids]
    
    if not new_tweets:
        logger.info(f"æ²¡æœ‰æ–°æ¨æ–‡ï¼ˆå…±æ£€æŸ¥äº† {len(tweets)} æ¡æ¨æ–‡ï¼‰")
        return 0
    
    logger.info(f"å‘ç° {len(new_tweets)} æ¡æ–°æ¨æ–‡ï¼Œå‡†å¤‡æ¨é€...")
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæ—§çš„åœ¨å‰ï¼Œæ–°çš„åœ¨åï¼‰
    # è¿™æ ·æ¨é€æ—¶å°±æ˜¯ä»æ—§åˆ°æ–°çš„é¡ºåº
    new_tweets.sort(key=lambda x: x['time'] if x['time'] else '')
    
    # æ¨é€æ–°æ¨æ–‡
    pushed_count = 0
    for idx, tweet in enumerate(new_tweets, 1):
        logger.info("-" * 60)
        logger.info(f"æ¨é€ç¬¬ {idx}/{len(new_tweets)} æ¡æ–°æ¨æ–‡")
        logger.info(f"ID: {tweet['id']}")
        if tweet['is_retweet']:
            logger.info(f"è½¬æ¨è€…: @{tweet['retweeter']['handle_raw']}")
            logger.info(f"åŸä½œè€…: @{tweet['original_author']['handle_raw']}")
        else:
            logger.info(f"ä½œè€…: @{tweet['original_author']['handle_raw']}")
        logger.info(f"æ—¶é—´: {tweet['time']}")
        
        # å‘é€åˆ°webhook
        success = await send_tweet_to_webhook(tweet)
        
        if success:
            # ä¿å­˜å·²æ¨é€ID
            save_pushed_id(tweet['id'])
            pushed_ids.add(tweet['id'])
            pushed_count += 1
        
        # æ¨æ–‡ä¹‹é—´é—´éš”
        if idx < len(new_tweets):
            await asyncio.sleep(2)
    
    logger.info(f"âœ“ æœ¬æ¬¡æˆåŠŸæ¨é€ {pushed_count} æ¡æ–°æ¨æ–‡")
    return pushed_count

def check_login_status(driver):
    """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
    try:
        driver.get("https://x.com/home")
        time.sleep(3)
        
        current_url = driver.current_url
        if "login" in current_url or "i/flow/login" in current_url:
            logger.warning("=" * 60)
            logger.warning("æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€")
            logger.warning("æµè§ˆå™¨å·²æ‰“å¼€ç™»å½•é¡µé¢ï¼Œè¯·åœ¨æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨ç™»å½•ä½ çš„Xè´¦å·ã€‚")
            logger.warning("ç™»å½•æˆåŠŸåï¼Œå›åˆ°è¿™é‡Œï¼ŒæŒ‰Enteré”®ç»§ç»­æ‰§è¡Œç›‘å¬...")
            logger.warning("æ³¨æ„ï¼šç™»å½•ä¿¡æ¯å°†ä¿å­˜åˆ°æœ¬åœ°ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶æ— éœ€é‡å¤ç™»å½•")
            logger.warning("=" * 60)
            input()
            return True
        else:
            logger.info("âœ“ æ£€æµ‹åˆ°å·²ç™»å½•çŠ¶æ€ï¼Œæ— éœ€é‡å¤ç™»å½•")
            return True
    except Exception as e:
        logger.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
        return False

async def monitor_lists_loop():
    """ä¸»ç›‘å¬å¾ªç¯"""
    logger.info("=" * 60)
    logger.info("æ¨ç‰¹åˆ—è¡¨ç›‘å¬è„šæœ¬å¯åŠ¨")
    logger.info("=" * 60)
    
    # åŠ è½½å·²æ¨é€çš„æ¨æ–‡ID
    pushed_ids = load_pushed_ids()
    logger.info(f"å·²æ¨é€çš„æ¨æ–‡æ•°: {len(pushed_ids)}")
    
    # å¯åŠ¨æµè§ˆå™¨
    driver = None
    try:
        driver = create_undetected_driver()
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not check_login_status(driver):
            logger.error("ç™»å½•æ£€æŸ¥å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            await send_error_to_webhook("ç™»å½•æ£€æŸ¥å¤±è´¥", "æ— æ³•ç™»å½•Twitterï¼Œç¨‹åºé€€å‡º")
            return
        
        logger.info(f"å¼€å§‹ç›‘å¬ {len(TWITTER_LISTS)} ä¸ªæ¨ç‰¹åˆ—è¡¨...")
        logger.info(f"æ£€æŸ¥é—´éš”: {LIST_CHECK_INTERVAL} ç§’")
        logger.info(f"æ¯æ¬¡æœ€å¤šè·å–: {MAX_TWEETS_PER_CHECK} æ¡æ¨æ–‡")
        logger.info("æç¤º: æŒ‰ Ctrl+C åœæ­¢ç›‘å¬")
        
        loop_count = 0
        # å¢åŠ ä¸€ä¸ªç™»å½•æ£€æŸ¥çš„è®¡æ•°å™¨
        login_check_interval_loops = 100  # æ¯100æ¬¡å¾ªç¯æ£€æŸ¥ä¸€æ¬¡ç™»å½•
        
        while True:
            loop_count += 1
            logger.info("#" * 60)
            logger.info(f"ç¬¬ {loop_count} æ¬¡æ£€æŸ¥")
            logger.info("#" * 60)
            
            # å®šæœŸæ£€æŸ¥ç™»å½•çŠ¶æ€
            if loop_count % login_check_interval_loops == 0:
                logger.info("å®šæœŸæ£€æŸ¥ç™»å½•çŠ¶æ€...")
                check_login_status(driver)
            
            total_new_tweets = 0
            
            # éå†æ‰€æœ‰åˆ—è¡¨
            for list_url in TWITTER_LISTS:
                try:
                    new_count = await monitor_list_once(driver, list_url, pushed_ids)
                    total_new_tweets += new_count
                    
                    # åˆ—è¡¨ä¹‹é—´çš„é—´éš”
                    if list_url != TWITTER_LISTS[-1]:
                        random_sleep(3, 6)
                        
                except Exception as e:
                    error_msg = f"ç›‘å¬åˆ—è¡¨å‡ºé”™: {e}"
                    logger.error(error_msg)
                    error_detail = traceback.format_exc()
                    logger.error(error_detail)
                    await send_error_to_webhook(error_msg, error_detail)
                    continue
            
            # æ‰“å°æœ¬è½®æ€»ç»“
            logger.info("=" * 60)
            logger.info(f"ç¬¬ {loop_count} æ¬¡æ£€æŸ¥å®Œæˆ")
            logger.info(f"æœ¬è½®å…±æ¨é€ {total_new_tweets} æ¡æ–°æ¨æ–‡")
            logger.info(f"ä¸‹æ¬¡æ£€æŸ¥æ—¶é—´: {LIST_CHECK_INTERVAL} ç§’å")
            logger.info("=" * 60)
            
            # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
            logger.debug(f"ç­‰å¾… {LIST_CHECK_INTERVAL} ç§’...")
            time.sleep(LIST_CHECK_INTERVAL)
            
    except KeyboardInterrupt:
        logger.info("\næ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
        
    except Exception as e:
        error_msg = f"ç›‘å¬è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"
        logger.critical(error_msg)
        error_detail = traceback.format_exc()
        logger.critical(error_detail)
        await send_error_to_webhook(error_msg, error_detail)
        
    finally:
        if driver:
            logger.info("å…³é—­æµè§ˆå™¨...")
            driver.quit()
        logger.info("ç›‘å¬å·²åœæ­¢")

# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == "__main__":
    # è¿è¡Œç›‘å¬å¾ªç¯
    asyncio.run(monitor_lists_loop())

